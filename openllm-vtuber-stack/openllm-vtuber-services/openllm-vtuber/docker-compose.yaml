services:
  openllm-vtuber:
    # image: ${IMAGE:-t41372/open-llm-vtuber}
    image: ${IMAGE:-t41372/open-llm-vtuber:with-bark-and-oai-whisper}
    container_name: ${NAME:-openllm-vtuber}
    restart: ${RESTART:-unless-stopped}

    build:
      context: ../DATA/Open-LLM-VTuber
      args:
        UID: ${PUID:-1000}
        GID: ${PGID:-1000}
        INSTALL_ORIGINAL_WHISPER: false
        INSTALL_BARK: false
      dockerfile: ./CustomDockerfile-openllm-vtuber-conda
      # dockerfile: ./CustomDockerfile-openllm-vtuber-uv
      # dockerfile: ./CustomDockerfile-openllm-vtuber-venv

    labels:
      - autoheal-app=true
      - autoheal
      - "com.centurylinklabs.watchtower.enable=true"

    volumes:
      - "../DATA/Open-LLM-VTuber/conf.yaml:/app/conf.yaml"
      - "../DATA/Open-LLM-VTuber/avatars:/app/avatars"
      - "../DATA/Open-LLM-VTuber/avatars:/app/backgrounds"
      - "../DATA/Open-LLM-VTuber/characters:/app/characters"
      - "../DATA/Open-LLM-VTuber/chat_history:/app/chat_history"
      - "../DATA/Open-LLM-VTuber/live2d-models:/app/live2d-models"
      - "../DATA/Open-LLM-VTuber/logs:/app/logs"
      - "../DATA/Open-LLM-VTuber/prompts/utils:/app/prompts/utils"
      - "../DATA/Open-LLM-VTuber/scripts:/app/scripts"
      - "../DATA/Open-LLM-VTuber/mcp_servers.json:/app/mcp_servers.json"
      - "../DATA/Open-LLM-VTuber/model_dict.json:/app/model_dict.json"

    hostname: ${NAME:-openllm-vtuber}

    # command: [ "ollama", "pull", "qwen3"]

    networks:
      host:
      openllm-vtuber-services:

    ports:
      - "${PORT:-12393}:12393"

    env_file:
      - ./openllm-vtuber/.env

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        # limits:
        #   memory: 1G
        #   cpus: "1.00"
        reservations:
          # memory: 512M
          # cpus: "0.50"
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, video, graphics, utility]

    # depends_on:
    #   - faster-whisper-gpu
    #   - wyoming-piper

    tty: true

    stdin_open: true

    # Optional: Health check for monitoring
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:12393/api/health",
        ]
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
