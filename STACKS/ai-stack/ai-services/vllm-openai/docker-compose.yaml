services:
  vllm-openai:
    image: ${IMAGE:-docker.io/vllm/vllm-openai:latest}
    restart: ${RESTART:-always}
    container_name: ${NAME:-vllm-openai}

    labels:
      - com.centurylinklabs.watchtower.enable=true
      - com.ouroboros.enable=true
      - autoheal-app
      - autoheal-app=true
      - homarr-app=true
      - homarr

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ:-Europe/Amsterdam}
      - PORT=${PORT:-3339}
      - OPENAI_API_KEY=sk-xxxx
      - VLLM_PORT=${PORT:-3339}
      - VLLM_TARGET_DEVICE=cuda

    ipc: host

    command: --model mistralai/Mistral-7B-v0.1

    volumes:
      # - /etc/localtime:/etc/localtime:ro
      # - /etc/timezone:/etc/timezone:ro
      - /var/run/docker.sock:/var/run/docker.sock

      - vllm-openai_huggingface_cache:/root/.cache/huggingface

    hostname: ${NAME:-vllm-openai}

    networks:
      host:
      ai-services:

    ports:
      - ${PORT:-3339}:3339

    env_file:
      - ./vllm-openai/.env

    deploy:
      replicas: 1

    tty: true

    stdin_open: true

    # healthcheck:
    #   test:
    #     [
    #       CMD,
    #       wget,
    #       --no-verbose,
    #       --tries=3,
    #       --spider,
    #       http://localhost:${PORT:-3339}/api/health,
    #     ]
    #   interval: 30s
    #   timeout: 10s
    #   start_period: 30s
    #   retries: 3

    shm_size: 1gb

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
