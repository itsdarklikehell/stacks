services:
  private-gpt-ollama:
    image: ${IMAGE:-zylonai/private-gpt:0.6.2-ollama}
    # pull_policy: always
    container_name: ${NAME:-private-gpt}
    restart: ${RESTART:-unless-stopped}

    user: ${PUID:-1000}:${PGID:-1000}

    # build:
    #   context: ../../../DATA/ai-stack/private-gpt
    #   args:
    #     UID: ${PUID:-1000}
    #     GID: ${PGID:-1000}
    #   dockerfile: ./Dockerfile

    profiles:
      - ""
      - ollama-cpu
      - ollama-cuda
      - ollama-api

    labels:
      - com.centurylinklabs.watchtower.enable=true
      - com.ouroboros.enable=true
      - autoheal-app
      - autoheal-app=true
      - homarr-app=true
      - homarr

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ:-Europe/Amsterdam}
      - PORT=${PORT:-8001}
      - HF_TOKEN=${HF_TOKEN}
      - PGPT_EMBED_MODE=ollama
      - PGPT_MODE=ollama
      - PGPT_OLLAMA_API_BASE=http://192.168.1.2:11434
      - PGPT_PROFILES=docker

    volumes:
      # - /etc/localtime:/etc/localtime:ro
      # - /etc/timezone:/etc/timezone:ro
      - /var/run/docker.sock:/var/run/docker.sock

      - private-gpt_ollama_local_data:/home/worker/app/local_data

    hostname: ${NAME:-private-gpt-ollama}

    networks:
      host:
      ai-services:

    ports:
      - ${PORT:-8001}:8001

    env_file:
      - ./private-gpt/.env

    # depends_on:
    #   - mongo-whispher
    #   - libretranslate-whispher

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video, graphics, utility]

    tty: true

    stdin_open: true

    # healthcheck:
    # test:
    #   [
    #     CMD,
    #     wget,
    #     --no-verbose,
    #     --tries=3,
    #     --spider,
    #     http://0.0.0.0:${PORT:-8082}/api/health,
    #   ]
    #   interval: 30s
    #   timeout: 10s
    #   start_period: 30s
    #   retries: 3

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3

    # depends_on:
    #   ollama:
    #     condition: service_healthy

  private-gpt-llamacpp-cpu:
    image: ${IMAGE:-zylonai/private-gpt:0.6.2-ollama}
    # pull_policy: always
    container_name: ${NAME:-private-gpt-llamacpp-cpu}
    restart: ${RESTART:-unless-stopped}

    user: ${PUID:-1000}:${PGID:-1000}

    # build:
    #   context: ../../../DATA/ai-stack/private-gpt
    #   args:
    #     UID: ${PUID:-1000}
    #     GID: ${PGID:-1000}
    #   dockerfile: ./Dockerfile

    profiles:
      - llamacpp-cpu

    labels:
      - com.centurylinklabs.watchtower.enable=true
      - com.ouroboros.enable=true
      - autoheal-app
      - autoheal-app=true
      - homarr-app=true
      - homarr

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ:-Europe/Amsterdam}
      - PORT=${PORT:-8001}

    volumes:
      # - /etc/localtime:/etc/localtime:ro
      # - /etc/timezone:/etc/timezone:ro
      - /var/run/docker.sock:/var/run/docker.sock

      - private-gpt_llamacpp-cpu_local_data:/home/worker/app/local_data
      - private-gpt_llamacpp-cpu_models/:/home/worker/app/models

    hostname: ${NAME:-private-gpt-llamacpp-cpu}

    networks:
      host:
      ai-services:

    ports:
      - ${PORT:-8001}:8001

    env_file:
      - ./private-gpt/.env

    # depends_on:
    #   - mongo-whispher
    #   - libretranslate-whispher

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video, graphics, utility]

    tty: true

    stdin_open: true

    # healthcheck:
    # test:
    #   [
    #     CMD,
    #     wget,
    #     --no-verbose,
    #     --tries=3,
    #     --spider,
    #     http://0.0.0.0:${PORT:-8082}/api/health,
    #   ]
    #   interval: 30s
    #   timeout: 10s
    #   start_period: 30s
    #   retries: 3

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
