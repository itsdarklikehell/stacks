services:
  localai:
    # image: ${IMAGE:-localai/localai:latest-gpu-nvidia-cuda-12}
    image: ${IMAGE:-localai/localai:latest-aio-gpu-nvidia-cuda-12}
    # image: ${IMAGE:-localai/localai:latest-aio-gpu-nvidia-cuda-11}
    # image: ${IMAGE:-localai/localai:latest}
    container_name: ${NAME:-localai}
    restart: ${RESTART:-unless-stopped}
    hostname: ${NAME:-localai}

    # build:
    #   context: ../../../DATA/ai-stack/localai
    #   dockerfile: Dockerfile
    #   args:
    #     - IMAGE_TYPE=aio
    #     - BASE_IMAGE=ubuntu:22.04
    #     - LOCALAI_ADDRESS=${IP_ADDRESS:-192.168.1.2}:${PORT:-8084}
    #     - LISTENING_ADDRESS=${IP_ADDRESS:-192.168.1.2}:${PORT:-8084}
    #     - BUILD_TYPE=cublas
    #     - LOCALAI_SINGLE_ACTIVE_BACKEND=true
    #     - THREADS=1
    #     - LOCALAI_IMAGE_PATH=/output
    #     - MODELS_PATH=/models
    #     - BACKENDS_PATH=/localai/backends
    #     - LOCALAI_IMAGE_PATH=/output
    #     - DEBUG=true

    labels:
      - com.centurylinklabs.watchtower.enable=true
      - com.ouroboros.enable=true
      - autoheal-app
      - autoheal-app=true
      - homarr-app=true
      - homarr

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ:-Europe/Amsterdam}

      - OAUTHLIB_RELAX_TOKEN_SCOPE=1
      - DOCKER_MODS=linuxserver/mods:universal-package-install
      - INSTALL_PACKAGES=libfuse2|git|gdb
      - RESTART_APP=true

      - PROFILE=gpu-8g
      - DEBUG=true

      - BACKENDS_PATH=/backends
      - GENERATED_CONTENT_PATH=/output
      - IMAGE_PATH=/output

      - LOCALAI_AUTOLOAD_BACKEND_GALLERIES=true
      - LOCALAI_AUTOLOAD_GALLERIES=true
      - LOCALAI_BACKENDS_PATH=/backends
      - LOCALAI_FEDERATED=true
      - LOCALAI_GENERATED_CONTENT_PATH=/output
      - LOCALAI_IMAGE_PATH=/output
      - LOCALAI_MODELS_PATH=/models
      - LOCALAI_P2P=true
      - FEDERATED_SERVER=true
      - LOCALAI_SINGLE_ACTIVE_BACKEND=true
      - LOCALAI_WATCHDOG_BUSY_TIMEOUT=5m
      - LOCALAI_WATCHDOG_BUSY=true
      - LOCALAI_WATCHDOG_IDLE_TIMEOUT=1h
      - LOCALAI_WATCHDOG_IDLE=true

    cap_add:
      - SYS_NICE
      - NET_ADMIN
      - IPC_LOCK

    # command: local-ai run --p2p
    # command: local-ai run --p2p --federated

    volumes:
      # - /etc/localtime:/etc/localtime:ro
      # - /etc/timezone:/etc/timezone:ro
      # - /var/run/docker.sock:/var/run/docker.sock

      - localai_cache:/.cache
      - localai_configuration:/configuration
      - localai_backends:/backends
      # - localai_content:/localai/content/images

      # MODELS:
      - localai_models:/models

      # - anything-llm_models:/models/anything-llm_models
      # - comfyui_models:/models/comfyui_models
      # - InvokeAI_models:/models/invokeai_models
      # - ollama_models:/models/ollama_models

      # OUTPUTS
      - anything-llm_output:/output/anything-llm_output
      - comfyui_output:/output/comfyui_output
      - InvokeAI_output:/output/InvokeAI_output
      - localai_output:/output

      # # INPUTS
      # - anything-llm_input:/input/anything-llm_input
      # - comfyui_input:/input/comfyui_input
      # - InvokeAI_input:/input/InvokeAI_input
      # - localai_input:/input

    # user: ${PUID:-1000}:${PGID:-1000}

    networks:
      host:
      ai-services:

    ports:
      - ${PORT:-8084}:8080

    env_file:
      - ./localai/.env

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video, graphics, utility]

    tty: true

    # stdin_open: true

    # shm_size: 1gb

    # healthcheck:
    # test:
    #   [
    #     CMD,
    #     wget,
    #     --no-verbose,
    #     --tries=3,
    #     --spider,
    #     http://localhost:${PORT:-8123}/api/health,
    #   ]
    #   interval: 30s
    #   timeout: 10s
    #   start_period: 30s
    #   retries: 3

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
