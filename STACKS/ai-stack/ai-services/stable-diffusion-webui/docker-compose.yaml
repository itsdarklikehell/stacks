services:
  stable-diffusion-webui:
    image: ${IMAGE:-tukirito/sygil-webui:latest}
    container_name: ${NAME:-stable-diffusion-webui}
    restart: ${RESTART:-unless-stopped}
    hostname: ${NAME:-stable-diffusion-webui}

    labels:
      - com.centurylinklabs.watchtower.enable=true
      - com.ouroboros.enable=true
      - autoheal-app
      - autoheal-app=true
      - homarr-app=true
      - homarr

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ:-Europe/Amsterdam}

      - IMAGE_MODEL=flux.1-dev-ggml
      - LISTENING_ADDRESS=8501
      - LLM_PROVIDER=ollama
      - LOCAL_AI_API_KEY=sk-123abc
      - LOCAL_AI_BASE_PATH=http://0.0.0.0:8083/v1
      - LOCAL_AI_MODEL_PREF=huggingfacetb_smollm3-3b
      - LOCAL_AI_MODEL_TOKEN_LIMIT=4096
      - MODEL_NAME=gemma-3-12b-it
      - MODELS_PATH=/models
      - MULTIMODAL_MODEL=moondream2-20250414
      - OLLAMA_BASE_PATH=http://${IP_ADDRESS}:11434
      - OLLAMA_HOST=http://0.0.0.0:11434
      - OLLAMA_MODEL_PREF=smallthinker:latest
      - OLLAMA_MODEL_TOKEN_LIMIT=4096
      - PORT=${PORT:-8501}
      - PROFILE=gpu-8g
      - STREAMLIT_SERVER_HEADLESS=true
      - SERVER_PORT=3001
      - VALIDATE_MODELS=false
      - WEBUI_SCRIPT=webui_streamlit.py
      - STABLE_DIFFUSION_WEBUI_BUILD_TYPE=cublas
      - STABLE_DIFFUSION_WEBUI_SINGLE_ACTIVE_BACKEND=true
      - STORAGE_DIR=/app/server/storage
      - USE_AIO=true

    cap_add:
      - SYS_NICE
      - NET_ADMIN
      - IPC_LOCK

    # command:
    # - granite-embedding-107m-multilingual
    # - moondream2-20250414
    # - huggingfacetb_smollm3-3b

    volumes:
      # - /etc/localtime:/etc/localtime:ro
      # - /etc/timezone:/etc/timezone:ro
      - /var/run/docker.sock:/var/run/docker.sock

      - stable-diffusion-webui_configdir:/.local
      - stable-diffusion-webui_models:/data/models
      - stable-diffusion-webui_output:/data/output

    # user: ${PUID:-1000}:${PGID:-1000}

    networks:
      host:
      ai-services:

    ports:
      - ${PORT:-8501}:8501

    env_file:
      - ./stable-diffusion-webui/.env

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video, graphics, utility]

    tty: true

    stdin_open: true

    healthcheck:
      test: [CMD, curl, -f, http://localhost:8083/readyz]
      interval: 1m
      timeout: 20m
      retries: 5

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
