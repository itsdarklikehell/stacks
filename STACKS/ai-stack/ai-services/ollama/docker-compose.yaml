services:
  ollama:
    image: ${IMAGE:-ollama/ollama:latest}
    container_name: ${NAME:-ollama}
    restart: ${RESTART:-unless-stopped}

    labels:
      - com.centurylinklabs.watchtower.enable=true
      - com.ouroboros.enable=true
      - autoheal-app
      - autoheal-app=true
      - homarr-app=true
      - homarr

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ:-Europe/Amsterdam}

      - OAUTHLIB_RELAX_TOKEN_SCOPE=1
      - DOCKER_MODS=linuxserver/mods:universal-package-install|linuxserver/mods:universal-apprise|linuxserver/mods:universal-cron|linuxserver/mods:universal-internationalization|linuxserver/mods:universal-stdout-logs|lscr.io/linuxserver/mods:universal-unrar6
      # - DOCKER_HOST=
      - INSTALL_PACKAGES=libfuse2|git|gdb
      - RESTART_APP=true

      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-1h}
      - ENABLE_IMAGE_GENERATION=${ENABLE_IMAGE_GENERATION:-True}
      - COMFYUI_BASE_URL=${COMFYUI_BASE_URL:-http://${IP_ADDRESS}:8188}
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_VULKAN=1

    volumes:
      - /etc/localtime:/etc/localtime:ro
      # - /etc/timezone:/etc/timezone:ro
      - /var/run/docker.sock:/var/run/docker.sock

      - ollama_data:/root/.ollama

      # MODELS:
      - ollama_models:/root/.ollama/models

      # - anything-llm_models:/root/.ollama/models/anything-llm_models
      # - comfyui_models:/root/.ollama/models/comfyui_models
      # - InvokeAI_models:/root/.ollama/models/invokeai_models
      # - localai_models:/root/.ollama/models

      # OUTPUTS
      - anything-llm_output:/root/.ollama/output/anything-llm_output
      - comfyui_output:/root/.ollama/output/comfyui_output
      - InvokeAI_output:/root/.ollama/output/InvokeAI_output
      - localai_output:/root/.ollama/output
      # # INPUTS
      # - anything-llm_input:/root/.ollama/input/anything-llm_input
      # - comfyui_input:/root/.ollama/input/comfyui_input
      # - InvokeAI_input:/root/.ollama/input/InvokeAI_input
      # - localai_input:/root/.ollama/input

    hostname: ${NAME:-ollama}

    # user: ${PUID:-1000}:${PGID:-1000}

    networks:
      host:
      ai-services:


    ports:
      - ${PORT:-11434}:11434

    env_file:
      - ./ollama/.env

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu, compute, video, graphics, utility ]

    tty: true

    stdin_open: true

    # shm_size: 1gb

    # healthcheck:
    #   test: [CMD, curl, -f, http://${IP_ADDRESS:-192.168.1.2}:11434/api/tags]
    #   interval: 10s
    #   timeout: 5s
    #   retries: 5

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
