services:
  # image used to:
  # scan, index, manage and serve:
  autoheal:
    container_name: autoheal-ai
    deploy:
      replicas: 1
    environment:
      AUTOHEAL_CONTAINER_LABEL: keep_healthy
    image: willfarrell/autoheal:latest
    network_mode: none
    restart: always
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/var/run/docker.sock
  # image used to:
  # scan, index, manage and serve:
  basic-memory:
    image: ghcr.io/basicmachines-co/basic-memory:latest
    container_name: basic-memory-server
    restart: unless-stopped
    hostname: basic-memory
    # Uncomment to build locally instead:
    # build: .
    volumes:
      - basic-memory-config:/root/.basic-memory:rw
      - ../DATA/basic-memory/knowledge:/app/data:rw
      # OPTIONAL: Mount additional knowledge directories for multiple projects
      # - ../DATA/basic-memory/work-notes:/app/data/work:rw
      # - ../DATA/basic-memory/personal-notes:/app/data/personal:rw
    environment:
      - BASIC_MEMORY_DEFAULT_PROJECT=main
      - BASIC_MEMORY_SYNC_CHANGES=true
      - BASIC_MEMORY_LOG_LEVEL=INFO
      - BASIC_MEMORY_SYNC_DELAY=1000
    networks:
      - ai-services
    env_file:
      - .env
    ports:
      - "8004:8004"
    command:
      [
        "basic-memory",
        "mcp",
        "--transport",
        "sse",
        "--host",
        "0.0.0.0",
        "--port",
        "8004",
      ]
    healthcheck:
      test: ["CMD", "basic-memory", "--version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

    # Optional: Resource limits
    # deploy:
    #   resources:
    #     limits:
    #       memory: 512M
    #       cpus: '0.5'
    #     reservations:
    #       memory: 256M
    #       cpus: '0.25'
  # image used to:
  # scan, index, manage and serve:
  faster-whisper-gpu:
    image: lscr.io/linuxserver/faster-whisper:gpu
    container_name: faster-whisper-gpu
    restart: unless-stopped
    hostname: faster-whisper-gpu
    networks:
      - ai-services
      - host
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - WHISPER_MODEL=tiny-int8
      - WHISPER_BEAM=1 #optional
      - WHISPER_LANG=en #optional
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/faster-whisper/data:/config
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  # image used to:
  # scan, index, manage and serve:
  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: homeassistant
    restart: unless-stopped
    hostname: homeassistant
    # network_mode: 'host'
    networks:
      iot_macvlan:
        ipv4_address: 192.168.1.11 #optional, I am using mac vlan, if you don't want to, remove iot_macvlan and don't create the network above
      ai-stack:
      management-stack:
      kuma_network:
      cloud:
    ports:
      - ${HASSPORT:-8123}:8123
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/home-assistant/config:/config
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
    depends_on:
      - faster-whisper-gpu
      - wyoming-piper
  # image used to:
  # scan, index, manage and serve:
  letta-db:
    image: ankane/pgvector:latest
    container_name: letta-db
    hostname: letta-db
    networks:
      default:
        aliases:
          - pgvector_db
          - letta-db
    env_file:
      - .env
    environment:
      - POSTGRES_USER=${LETTA_PG_USER:-letta}
      - POSTGRES_PASSWORD=${LETTA_PG_PASSWORD:-letta}
      - POSTGRES_DB=${LETTA_PG_DB:-letta}
    volumes:
      - ../DATA/letta/.persist/pgdata:/var/lib/postgresql/data
      # - ~/.letta/.persist/pgdata:/var/lib/postgresql/data
      - ../DATA/letta/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - ${LETTA_PG_PORT:-5432}:5432
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U letta"]
      interval: 5s
      timeout: 5s
      retries: 5
  # image used to:
  # scan, index, manage and serve:
  letta-mcp:
    image: ghcr.io/oculairmedia/letta-mcp-server:latest
    container_name: letta-mcp
    hostname: letta-mcp
    restart: unless-stopped
    ports:
      - ${LETTA_MCP_PORT:-3001}:3001
    env_file:
      - .env
    environment:
      - LETTA_BASE_URL=${LETTA_BASE_URL:-http://host.docker.internal:8283}
      - LETTA_PASSWORD=${LETTA_PASSWORD}
      - NODE_ENV=${NODE_ENV:-production}
  # image used to:
  # scan, index, manage and serve:
  letta-server:
    image: letta/letta:latest
    container_name: letta-server
    restart: unless-stopped
    hostname: letta-server
    # network_mode: host
    networks:
      default:
        aliases:
          - pgvector_db
          - letta-db
    depends_on:
      letta-db:
        condition: service_healthy
    ports:
      - 8283:8283
      - 8083:8083
    env_file:
      - .env
    environment:
      - LETTA_PG_URI=${LETTA_PG_URI:-postgresql://${LETTA_PG_USER:-letta}:${LETTA_PG_PASSWORD:-letta}@${LETTA_DB_HOST:-letta-db}:${LETTA_PG_PORT:-5432}/${LETTA_PG_DB:-letta}}
      - LETTA_DEBUG=${LETTA_DEBUG:-True}
      - SKIP_INIT_DB=${SKIP_INIT_DB:-True}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - AZURE_API_KEY=${AZURE_API_KEY}
      - AZURE_BASE_URL=${AZURE_BASE_URL}
      - AZURE_API_VERSION=${AZURE_API_VERSION}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - VLLM_API_BASE=${VLLM_API_BASE}
      - OPENLLM_AUTH_TYPE=${OPENLLM_AUTH_TYPE}
      - OPENLLM_API_KEY=${OPENLLM_API_KEY}
      - LETTA_OTEL_EXPORTER_OTLP_ENDPOINT=${LETTA_OTEL_EXPORTER_OTLP_ENDPOINT}
      - CLICKHOUSE_ENDPOINT=${CLICKHOUSE_ENDPOINT}
      - CLICKHOUSE_DATABASE=${CLICKHOUSE_DATABASE}
      - CLICKHOUSE_USERNAME=${CLICKHOUSE_USERNAME}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
    volumes:
      # - ../DATA/letta/configs/server_config.yaml:/root/.letta/config # config file
      # - ../DATA/letta/credentials:/root/.letta/credentials # credentials file
      # Uncomment this line to mount a local directory for tool execution, and specify the mount path
      # before running docker compose: `export LETTA_SANDBOX_MOUNT_PATH=$PWD/directory`
      - ${LETTA_SANDBOX_MOUNT_PATH:-/letta}:/root/.letta/tool_execution_dir # mounted volume for tool execution
  # image used to:
  # scan, index, manage and serve:
  libretranslate:
    container_name: libretranslate
    image: libretranslate/libretranslate:latest-cuda
    hostname: libretranslate
    restart: unless-stopped
    env_file:
      - .env
    networks:
      - ai-services
      - host
    volumes:
      - /etc/localtime:/etc/localtime:ro
      # - /etc/timezone:/etc/timezone:ro
      - ../DATA/whisper/libretranslate/data:/home/libretranslate/.local/share
      - ../DATA/whisper/libretranslate/cache:/home/libretranslate/.local/cache
    user: root
    tty: true
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - LT_DISABLE_WEB_UI=True
      - LT_LOAD_ONLY=${LT_LOAD_ONLY:-en,fr,es}
      - LT_UPDATE_MODELS=True
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - ${LIBRETRANSLATE_PORT:-5000}:5000
  # image used to:
  # scan, index, manage and serve:
  mongo:
    image: mongo:latest
    container_name: mongo
    hostname: mongo
    env_file:
      - .env
    networks:
      - ai-services
      - host
    restart: unless-stopped
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/whisper/db_data:/data/db
      - ../DATA/whisper/db_data/logs/:/var/log/mongodb/
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - MONGO_INITDB_ROOT_USERNAME=${DB_USER:-whishper}
      - MONGO_INITDB_ROOT_PASSWORD=${DB_PASS:-whishper}
    command: ["--logpath", "/var/log/mongodb/mongod.log"]
    ports:
      - ${MONGODB_PORT:-27017}:27017
  # image used to:
  # scan, index, manage and serve:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    hostname: ollama
    restart: unless-stopped
    networks:
      - ai-services
      - host
    env_file:
      - .env
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - OLLAMA_KEEP_ALIVE=24h
      - ENABLE_IMAGE_GENERATION=True
      - COMFYUI_BASE_URL=http://comfyui:7860
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/ollama:/root/.ollama
    ports:
      - ${OLLAMA_PORT:-11434}:11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  # image used to:
  # scan, index, manage and serve:
  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui
    hostname: open-webui
    restart: unless-stopped
    networks:
      - ai-services
      - host
    env_file:
      - .env
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-11434}
      - ENABLE_RAG_WEB_SEARCH=True
      - RAG_WEB_SEARCH_ENGINE=searxng
      - RAG_WEB_SEARCH_RESULT_COUNT=3
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=10
      - SEARXNG_QUERY_URL=http://searxng:8080/search?q=<query>
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/open-webui:/app/backend/data
    depends_on:
      - ollama
    extra_hosts:
      - host.docker.internal:host-gateway
    ports:
      - ${OPEN_WEBUI_PORT:-8080}:8080
  # image used to:
  # scan, index, manage and serve:
  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    hostname: searxng
    restart: unless-stopped
    networks:
      - ai-services
      - host
    env_file:
      - .env
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/searxng:/etc/searxng
    ports:
      - ${SEARXNG_PORT:-8081}:8080
  # image used to:
  # scan, index, manage and serve:
  stable-diffusion-models-download:
    build: ./stable-diffusion-webui-docker/services/download/
    image: comfy-download:latest
    container_name: stable-diffusion-models-download
    hostname: stable-diffusion-models-download
    networks:
      - ai-services
      - host
    env_file:
      - .env
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/stable-diffusion-webui-docker/data:/data
  # image used to:
  # scan, index, manage and serve:
  stable-diffusion-webui:
    build: ./stable-diffusion-webui-docker/services/comfy/
    image: comfy-ui:latest
    container_name: stable-diffusion-webui
    hostname: stable-diffusion-webui
    networks:
      - ai-services
      - host
    env_file:
      - .env
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - CLI_ARGS=
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/stable-diffusion-webui-docker/data:/data
      - ../DATA/stable-diffusion-webui-docker/output:/output
    stop_signal: SIGKILL
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [compute, utility]
    restart: unless-stopped
    ports:
      - ${COMFYUI_PORT:-7860}:7860
  # image used to:
  # scan, index, manage and serve:
  # image used to:
  # scan, index, manage and serve:
  # image used to:
  # scan, index, manage and serve:
  swarmui:
    image: swarmui
    container_name: swarmui
    user: ${HOST_UID:-1000}:${HOST_GID:-1000}
    restart: unless-stopped
    hostname: swarmui
    networks:
      - ai-services
      - host
    env_file:
      - .env
    cap_drop:
      - ALL
    build:
      context: .
      args:
        UID: ${HOST_UID:-1000}
      dockerfile: swarmui/launchtools/CustomDockerfile.docker
    # uncomment `network_mode: host` if you want to access other services on the host network (eg a separated comfy instance)
    # network_mode: host
    volumes:
      - swarmdata:/SwarmUI/Data
      - swarmbackend:/SwarmUI/dlbackend
      - swarmdlnodes:/SwarmUI/src/BuiltinExtensions/ComfyUIBackend/DLNodes
      - swarmextensions:/SwarmUI/src/Extensions
      - ../DATA/swarmui/Models:/SwarmUI/Models
      - ../DATA/swarmui/Output:/SwarmUI/Output
      - ../DATA/swarmui/src/BuiltinExtensions/ComfyUIBackend/CustomWorkflows:/SwarmUI/src/BuiltinExtensions/ComfyUIBackend/CustomWorkflows
    ports:
      - "7801:7801"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # change the count to the number of GPUs you want to use.
              count: 1
              capabilities: [gpu]
  whisper:
    container_name: whisper
    pull_policy: always
    image: pluja/whishper:latest-gpu
    hostname: whisper
    restart: unless-stopped
    env_file:
      - .env
    networks:
      - ai-services
      - host
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/whisper/uploads:/app/uploads
      - ../DATA/whisper/logs:/var/log/whishper
      - ../DATA/whisper/models:/app/models
    depends_on:
      - mongo
      - libretranslate
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - PUBLIC_INTERNAL_API_HOST=${WHISHPER_HOST}
      - PUBLIC_TRANSLATION_API_HOST=${WHISHPER_HOST}
      - PUBLIC_API_HOST=${WHISHPER_HOST:-}
      - PUBLIC_WHISHPER_PROFILE=gpu
      - WHISPER_MODELS_DIR=/app/models
      - UPLOAD_DIR=/app/uploads
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - ${WHISPER_PORT:-8000}:80
      - 10300:10300
  # image used to:
  # scan, index, manage and serve:
  wyoming-piper:
    image: rhasspy/wyoming-piper # no gpu
    container_name: wyoming-piper
    restart: unless-stopped
    hostname: wyoming-piper
    networks:
      - ai-services
      - host
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/wyoming-piper/data:/data
    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
    command: --voice en_US-lessac-medium
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - ${PIPER_PORT:-10200}:10200
  # image used to:
  # scan, index, manage and serve:

  # image used to:
  # scan, index, manage and serve:

  # open-llm-vtuber:
  #   image: "t41372/open-llm-vtuber:latest"
  #   container_name: open-llm-vtuber
  #   hostname: open-llm-vtuber
  #   restart: unless-stopped
  #   user: ${HOST_UID:-1000}:${HOST_GID:-1000}
  #   build:
  #     context: .
  #     args:
  #       UID: ${HOST_UID:-1000}
  #     dockerfile: open-llm-vtuber/CustomDockerfile.docker
  #   networks:
  #     - ai-stack
  #   env_file:
  #     - .env
  #   ports:
  #     - ${OLLMVTUBER_PORT:-12393}:12393
  #   volumes:
  #     - ../DATA/open-llm-vtuber/conf.yaml:/app/conf.yaml:ro

  watchtower:
    image: "containrrr/watchtower:latest"
    container_name: watchtower-ai-stack
    restart: unless-stopped
    hostname: watchtower
    networks:
      - ai-services
      - host
    env_file:
      - .env
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    # command: --debug --http-api-update
    # environment:
    #   - WATCHTOWER_HTTP_API_TOKEN=${WATCHTOWER_HTTP_API_TOKEN}
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
    ports:
      - ${WATCHTOWER_PORT:-8082}:8080
    depends_on:
      - autoheal
      - basic-memory
      - faster-whisper-gpu
      - homeassistant
      - letta-db
      - letta-mcp
      - letta-server
      - libretranslate
      - mongo
      - ollama
      - open-webui
      - searxng
      - stable-diffusion-models-download
      - stable-diffusion-webui
      - swarmui
      - whisper
      - wyoming-piper
      # - watchtower

volumes:
  # Named volume for persistent configuration and database
  # This ensures your configuration and knowledge graph persist across container restarts
  basic-memory-config:
    driver: local
  swarmdata:
  swarmbackend:
  swarmdlnodes:
  swarmextensions:

networks:
  ai-stack:
    external: true
  iot_macvlan:
    external: true
  basic-memory-net:
  management-stack:
    external: true
  gaseous:
    driver: bridge
  proxy:
    external: true
  cloud:
    name: cloud
    driver: bridge
    external: true
  zabbix-network:
    external: true
  kuma_network:
    driver: bridge
    external: true
  wg:
    driver: bridge
    enable_ipv6: true
    ipam:
      driver: default
      config:
        - subnet: 10.42.42.0/24
        - subnet: fdcc:ad94:bacf:61a3::/64
