services:
  ollama:
    image: ${IMAGE:-ollama/ollama:latest}
    container_name: ${NAME:-ollama}
    restart: ${RESTART:-unless-stopped}

    labels:
      - com.centurylinklabs.watchtower.enable=true
      # - autoheal-app
      # - autoheal-app=true

    environment:
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-24h}
      - ENABLE_IMAGE_GENERATION=${ENABLE_IMAGE_GENERATION:-True}
      - COMFYUI_BASE_URL=${COMFYUI_BASE_URL:-http://stable-diffusion-webui:7860}
      - OLLAMA_MODELS=${OLLAMA_MODELS:-/root/.ollama/models}

    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - /var/run/docker.sock:/var/run/docker.sock

      - ollama_data:/root/.ollama

    hostname: ${NAME:-ollama}

    networks:
      host:
      ai-services:

    ports:
      - ${PORT:-11434}:11434

    env_file:
      - ./ollama/.env

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video, graphics, utility]

    tty: true

    stdin_open: true

    # healthcheck:
    # test:
    #   [
    #     CMD,
    #     wget,
    #     --no-verbose,
    #     --tries=3,
    #     --spider,
    #     http://192.168.1.2:11434/api/health,
    #   ]
    #   interval: 30s
    #   timeout: 10s
    #   start_period: 30s
    #   retries: 3

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
