services:
  ollama:
    image: ${IMAGE:-ollama/ollama}
    container_name: ${NAME:-ollama}
    restart: ${RESTART:-unless-stopped}

    labels:
      - autoheal-app=true
      - autoheal
      - "com.centurylinklabs.watchtower.enable=true"

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - OLLAMA_KEEP_ALIVE=24h
      - ENABLE_IMAGE_GENERATION=True
      - COMFYUI_BASE_URL=http://comfyui:7860
      - OLLAMA_MODELS=/root/.ollama/models

    volumes:
      - ollama-data:/root/.ollama
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - ../DATA/ollama:/root/.ollama

    hostname: ${NAME:-ollama}

    networks:
      host:
      ai-services:

    ports:
      - "${PORT:-11434}:11434"

    env_file:
      - ./ollama/.env

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        # limits:
        #   memory: 1G
        #   cpus: "1.00"
        reservations:
          # memory: 512M
          # cpus: "0.50"
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, video, graphics, utility]

    tty: true
    
    stdin_open: true

    # Optional: Health check for monitoring
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:11434/api/health"]
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"