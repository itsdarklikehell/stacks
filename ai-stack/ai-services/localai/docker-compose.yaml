services:
  localai-localai:
    # image: ${IMAGE:-localai/localai:latest-gpu-nvidia-cuda-12}
    image: ${IMAGE:-localai/localai:latest-aio-gpu-nvidia-cuda-12}
    # image: localai/localai:latest-aio-gpu-nvidia-cuda-11
    # image: localai/localai:latest-aio-gpu-nvidia-cuda-12
    # image: localai/localai:latest
    container_name: ${NAME:-localai-localai}
    restart: ${RESTART:-unless-stopped}

    labels:
      - com.centurylinklabs.watchtower.enable=true
      # - autoheal-app
      # - autoheal-app=true

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ:-Europe/Amsterdam}
      - DEBUG=true
      - STORAGE_DIR=/app/server/storage
      - SERVER_PORT=${SERVER_PORT:-8083}
      - PORT=${PORT:-8083}
      - MODELS_PATH=/models
      - THREADS=1
      # - PROFILE=gpu-8g

    cap_add:
      - SYS_NICE
      - NET_ADMIN
      - IPC_LOCK

    command:
      - granite-embedding-107m-multilingual

    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      # - localai_models:/models:cached
      # - localai_backends:/backends:cached
      # - localai_images:/images:cached
      - ../../DATA/ai-stack/localai/models:/models:cached
      - ../../DATA/ai-stack/localai/backends:/backends:cached
      - ../../DATA/ai-stack/localai/images:/tmp/generated/images
      # - type: bind
      #   source: ../../DATA/ai-stack/localai/models
      #   target: /models
      # - type: bind
      #   source: ../../DATA/ai-stack/localai/backends
      #   target: /backends
      # - type: bind
      #   source: ../../DATA/ai-stack/localai/images
      #   target: /tmp/generated/images

    hostname: ${NAME:-localai}

    user: ${PUID:-1000}:${PGID:-1000}

    networks:
      host:
      ai-services:

    ports:
      - ${PORT:-8083}:8083

    env_file:
      - ./localai/.env

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video, graphics, utility]

    tty: true

    stdin_open: true

    healthcheck:
      test: [CMD, curl, -f, http://localhost:8083/readyz]
      interval: 1m
      timeout: 20m
      retries: 5

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3

  localai-localrecall:
    image: quay.io/mudler/localrecall
    container_name: localai-localrecall
    restart: unless-stopped

    build:
      context: ../../DATA/ai-stack/localrecall
      dockerfile: Dockerfile

    ports:
      - 8080:8080

    env_file:
      - ./localai/.env

    environment:
      - COLLECTION_DB_PATH=/db
      - EMBEDDING_MODEL=granite-embedding-107m-multilingual
      - FILE_ASSETS=/assets
      - OPENAI_API_KEY=sk-1234567890
      - OPENAI_BASE_URL=http://localai-localai:8080
      - LISTENING_ADDRESS=8080
      - FILE_ASSETS=/assets

    volumes:
      - ../../DATA/ai-stack/localrecall/volumes/db:/db
      - ../../DATA/ai-stack/localrecall/volumes/assets/:/assets

  localrecall-healthcheck:
    image: busybox
    restart: unless-stopped
    container_name: localrecall-healthcheck

    depends_on:
      localai-localrecall:
        condition: service_started

    command:
      [
        sh,
        -c,
        until wget -q -O - http://localai-localrecall:8080 > /dev/null 2>&1; do echo 'Waiting for localrecall...'; sleep 1; done; echo 'localrecall is up!',
      ]

  localai-dind:
    image: docker:dind
    container_name: localai-dind
    restart: unless-stopped

    privileged: true

    environment:
      - DOCKER_TLS_CERTDIR=

    healthcheck:
      test: [CMD, docker, info]
      interval: 10s
      timeout: 5s
      retries: 3

  localai-localagi:
    container_name: localai-localagi
    restart: unless-stopped
    image: quay.io/mudler/localagi:master

    depends_on:
      localai-localai:
        condition: service_healthy
      localrecall-healthcheck:
        condition: service_completed_successfully
      localai-dind:
        condition: service_healthy

    build:
      context: ../../DATA/ai-stack/localagi
      dockerfile: Dockerfile.webui

    ports:
      - 8080:3000

    environment:
      - LOCALAGI_MODEL=${MODEL_NAME:-gemma-3-4b-it-qat}
      - LOCALAGI_MULTIMODAL_MODEL=${MULTIMODAL_MODEL:-moondream2-20250414}
      - LOCALAGI_IMAGE_MODEL=${IMAGE_MODEL:-sd-1.5-ggml}
      - LOCALAGI_LLM_API_URL=http://localai-localagi:8080
      #- LOCALAGI_LLM_API_KEY=sk-1234567890
      - LOCALAGI_LOCALRAG_URL=http://localai-localrecall:8080
      - LOCALAGI_STATE_DIR=/pool
      - LOCALAGI_TIMEOUT=5m
      - LOCALAGI_ENABLE_CONVERSATIONS_LOGGING=false
      - LOCALAGI_SSHBOX_URL=root:root@sshbox:22
      - DOCKER_HOST=tcp://localai-dind:2375

    extra_hosts:
      - host.docker.internal:host-gateway

    volumes:
      - ../../DATA/ai-stack/localagi/volumes:/pool

  localai-sshbox:
    container_name: localai-sshbox
    restart: unless-stopped

    build:
      context: ../../DATA/ai-stack/localagi
      dockerfile: Dockerfile.sshbox

    ports:
      - 22

    environment:
      - SSH_USER=root
      - SSH_PASSWORD=root
      - DOCKER_HOST=tcp://localai-dind:2375

    depends_on:
      localai-dind:
        condition: service_healthy
