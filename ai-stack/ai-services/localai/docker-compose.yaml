services:
  localai:
    image: "${IMAGE:-localai/localai:latest-aio-gpu-nvidia-cuda-12}"
    # image: "${IMAGE:-localai/localai:latest-gpu-nvidia-cuda-12}"
    container_name: ${NAME:-localai}
    restart: ${RESTART:-unless-stopped}

    labels:
      - "autoheal-app=true"
      - "autoheal"
      - "com.centurylinklabs.watchtower.enable=true"

    environment:
      - "PUID=${PUID:-1000}"
      - "PGID=${PGID:-1000}"
      - "TZ=${TZ:-Europe/Amsterdam}"
      - 'STORAGE_DIR="/app/server/storage"'
      - "SERVER_PORT=8080"
      - "DEBUG=true"


    cap_add:
      - "SYS_ADMIN"

    command: ["--cap-add", "SYS_ADMIN"]

    volumes:
      # - ../DATA/localai/models:/models:cached
      - localai-models:/models
      # - "localai_storage:${STORAGE_DIR:-/app/server/storage}"
      # - "localai_hotdir:/app/collector/hotdir"
      # - "localai_outputs:/app/collector/outputs"
      # - "../DATA/localai/localai_storage:${STORAGE_DIR:-/app/server/storage}"
      # - "../DATA/localai/localai_hotdir:/app/collector/hotdir"
      # - "../DATA/localai/localai_outputs:/app/collector/outputs"


    hostname: "${NAME:-localai}"

    user: "${PUID:-1000}:${PGID:-1000}"

    networks:
      host:
      ai-services:

    ports:
      - "${PORT:-8080}:8080"

    env_file:
      - "./localai/.env"

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        # limits:
        #   memory: 1G
        #   cpus: "1.00"
        reservations:
          # memory: 512M
          # cpus: "0.50"
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, video, graphics, utility]

    tty: true
    
    stdin_open: true

    # Optional: Health check for monitoring
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 1m
      timeout: 20m
      retries: 5
      start_period: "30s"
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"