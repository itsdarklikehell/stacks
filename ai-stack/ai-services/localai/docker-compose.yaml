services:
  localai:
    image: "${IMAGE:-localai/localai:latest-aio-gpu-nvidia-cuda-12}"
    # image: "${IMAGE:-localai/localai:latest-gpu-nvidia-cuda-12}"
    container_name: "${NAME:-localai}"
    restart: "${RESTART:-unless-stopped}"

    labels:
      - "com.centurylinklabs.watchtower.enable=true"
      # - "autoheal-app=true"
      # - "autoheal"

    environment:
      - "DEBUG=true"
      - 'STORAGE_DIR="/app/server/storage"'
      # - "PUID=${PUID:-1000}"
      # - "PGID=${PGID:-1000}"
      # - "TZ=${TZ:-Europe/Amsterdam}"
      # - "SERVER_PORT=8080"

    # cap_add:
    #   - "SYS_ADMIN"

    # command: ["--cap-add", "SYS_ADMIN"]

    volumes:
      - localai_models:/models
      # - ../DATA/localai/models:/models:cached

    hostname: "${NAME:-localai}"

    user: "${PUID:-1000}:${PGID:-1000}"

    networks:
      host:
      ai-services:

    ports:
      - "${PORT:-8083}:8080"

    env_file:
      - "./localai/.env"

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        # limits:
        #   memory: 1G
        #   cpus: "1.00"
        reservations:
          # memory: 512M
          # cpus: "0.50"
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, video, graphics, utility]

    # tty: true
    
    # stdin_open: true

    # Optional: Health check for monitoring
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 1m
      timeout: 20m
      retries: 5
      start_period: "30s"
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"