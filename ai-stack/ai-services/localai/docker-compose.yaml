services:
  localai:
    # image: ${IMAGE:-localai/localai:latest-gpu-nvidia-cuda-12}
    image: ${IMAGE:-localai/localai:latest-aio-gpu-nvidia-cuda-12}
    # image: ${IMAGE:-localai/localai:latest-aio-gpu-nvidia-cuda-11}
    # image: ${IMAGE:-localai/localai:latest}
    # image: ${IMAGE:-quay.io/go-skynet/local-ai:master}
    container_name: ${NAME:-localai}
    restart: ${RESTART:-unless-stopped}
    hostname: ${NAME:-localai}

    # build:
    #   context: ../../DATA/ai-stack/localai
    #   dockerfile: Dockerfile
    #   args:
    #     - IMAGE_TYPE=aio
    #     - BASE_IMAGE=ubuntu:22.04
    #     - LOCALAI_ADDRESS=0.0.0.0:${PORT:-8083}
    #     - LISTENING_ADDRESS=0.0.0.0:${PORT:-8083}
    #     # - BUILD_TYPE=cublas
    #     - LOCALAI_IMAGE_PATH=/tmp/generated/content/images
    #     - LOCALAI_SINGLE_ACTIVE_BACKEND=true

    labels:
      - com.centurylinklabs.watchtower.enable=true
      # - autoheal-app
      # - autoheal-app=true

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ:-Europe/Amsterdam}
      - DEBUG=true
      - STORAGE_DIR=/app/server/storage
      - SERVER_PORT=${SERVER_PORT:-8083}
      - LOCALAI_ADDRESS=0.0.0.0:${PORT:-8083}
      - LISTENING_ADDRESS=0.0.0.0:${PORT:-8083}
      - PORT=${PORT:-8083}
      - MODELS_PATH=/models
      - THREADS=1
      # - PROFILE=gpu-8g

    cap_add:
      - SYS_NICE
      - NET_ADMIN
      - IPC_LOCK

    # command:
    # - granite-embedding-107m-multilingual
    # - moondream2-20250414
    # - huggingfacetb_smollm3-3b

    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro

      - ../../DATA/ai-stack/localai/.cache:/.cache:cached
      # - localai_cache:/.cache:cached

      - ../../DATA/ai-stack/localai/backends:/usr/share/localai/backends:cached
      # - localai_backends:/usr/share/localai/backends:cached

      - ../../DATA/ai-stack/localai/configuration:/configuration:cached
      # - localai_configuration:/configuration:cached

      - ../../DATA/ai-stack/localai/content:/tmp/generated/content:cached
      # - localai_content:cached

      - ../../DATA/ai-stack/localai/images:/tmp/generated/content/images:cached
      # - localai_images:cached

      - ../../DATA/ai-stack/localai/models:/models:cached
      # - localai_models:/models:cached

    user: ${PUID:-1000}:${PGID:-1000}

    networks:
      host:
      ai-services:

    ports:
      - ${PORT:-8083}:8083

    env_file:
      - ./localai/.env

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video, graphics, utility]

    tty: true

    stdin_open: true

    healthcheck:
      test: [CMD, curl, -f, http://localhost:8083/readyz]
      interval: 1m
      timeout: 20m
      retries: 5

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3

  # localrecall:
  #   image: quay.io/mudler/localrecall
  #   container_name: localrecall
  #   restart: unless-stopped

  #   build:
  #     context: ../../DATA/ai-stack/localrecall
  #     dockerfile: Dockerfile

  #   ports:
  #     - 8084:8084

  #   env_file:
  #     - ./localai/.env

  #   environment:
  #     - COLLECTION_DB_PATH=/db
  #     - EMBEDDING_MODEL=granite-embedding-107m-multilingual
  #     - FILE_ASSETS=/assets
  #     # - OPENAI_API_KEY=sk-1234567890
  #     - OPENAI_BASE_URL=http://localai:${PORT:-8083}
  #     - LISTENING_ADDRESS=0.0.0.0:8084
  #     - FILE_ASSETS=/assets

  #   volumes:
  #     - ../../DATA/ai-stack/localrecall/volumes/db:/db
  #     - ../../DATA/ai-stack/localrecall/volumes/assets/:/assets

  # localrecall-healthcheck:
  #   image: busybox
  #   restart: unless-stopped
  #   container_name: localrecall-healthcheck

  #   depends_on:
  #     localrecall:
  #       condition: service_started

  #   command:
  #     [
  #       sh,
  #       -c,
  #       until wget -q -O - http://localrecall:8084 > /dev/null 2>&1; do echo 'Waiting for localrecall...'; sleep 1; done; echo 'localrecall is up!',
  #     ]

  # dind:
  #   image: docker:dind
  #   container_name: dind
  #   restart: unless-stopped

  #   privileged: true

  #   environment:
  #     - DOCKER_TLS_CERTDIR=

  #   ports:
  #     - 2358:2358

  #   healthcheck:
  #     test: [CMD, docker, info]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3

  # localagi:
  #   container_name: localagi
  #   restart: unless-stopped
  #   image: quay.io/mudler/localagi:master

  #   depends_on:
  #     localai:
  #       condition: service_healthy
  #     localrecall-healthcheck:
  #       condition: service_completed_successfully
  #     dind:
  #       condition: service_healthy

  #   build:
  #     context: ../../DATA/ai-stack/localagi
  #     dockerfile: Dockerfile.webui

  #   ports:
  #     - 8085:8085

  #   environment:
  #     - LOCALAGI_MODEL=${MODEL_NAME:-gemma-3-4b-it-qat}
  #     - LOCALAGI_MULTIMODAL_MODEL=${MULTIMODAL_MODEL:-moondream2-20250414}
  #     - LOCALAGI_IMAGE_MODEL=${IMAGE_MODEL:-sd-1.5-ggml}
  #     - LOCALAGI_LLM_API_URL=http://localagi:8085
  #     #- LOCALAGI_LLM_API_KEY=sk-1234567890
  #     - LOCALAGI_LOCALRAG_URL=http://localrecall:8085
  #     - LOCALAGI_STATE_DIR=/pool
  #     - LOCALAGI_TIMEOUT=5m
  #     - LOCALAGI_ENABLE_CONVERSATIONS_LOGGING=true
  #     - LOCALAGI_SSHBOX_URL=root:root@sshbox:2222
  #     - DOCKER_HOST=tcp://dind:2358

  #   extra_hosts:
  #     - host.docker.internal:host-gateway

  #   volumes:
  #     - ../../DATA/ai-stack/localagi/volumes:/pool

  # sshbox:
  #   container_name: sshbox
  #   restart: unless-stopped

  #   build:
  #     context: ../../DATA/ai-stack/localagi
  #     dockerfile: Dockerfile.sshbox

  #   ports:
  #     - 2222:2222

  #   environment:
  #     - SSH_USER=root
  #     - SSH_PASSWORD=root
  #     - DOCKER_HOST=tcp://dind:2358

  #   depends_on:
  #     dind:
  #       condition: service_healthy
