services:
  localai:
    # image: ${IMAGE:-localai/localai:latest-gpu-nvidia-cuda-12}
    image: ${IMAGE:-localai/localai:latest-aio-gpu-nvidia-cuda-12}
    container_name: ${NAME:-localai}
    restart: ${RESTART:-unless-stopped}

    labels:
      - com.centurylinklabs.watchtower.enable=true
      # - autoheal-app
      # - autoheal-app=true

    environment:
      - PUID=${PUID:-1000}
      - PGID=${PGID:-1000}
      - TZ=${TZ:-Europe/Amsterdam}
      - DEBUG=true
      - STORAGE_DIR=/app/server/storage
      - SERVER_PORT=${SERVER_PORT:-8083}
      - PORT=${PORT:-8083}
      - MODELS_PATH=/models
      - THREADS=1
      # - PROFILE=gpu-8g

    cap_add:
      - SYS_NICE
      - NET_ADMIN
      - IPC_LOCK

    # command: [--cap-add=SYS_NICE, --cap-add=NET_ADMIN, --cap-add=IPC_LOCK]

    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
      # - localai_models:/models:cached
      - ../../DATA/ai-stack/localai/models:/models:cached

    hostname: ${NAME:-localai}

    user: ${PUID:-1000}:${PGID:-1000}

    networks:
      host:
      ai-services:

    ports:
      - ${PORT:-8083}:8080

    env_file:
      - ./localai/.env

    runtime: nvidia
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    tty: true

    stdin_open: true

    healthcheck:
      test: [CMD, curl, -f, http://localhost:8083/readyz]
      interval: 1m
      timeout: 20m
      retries: 5

    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
