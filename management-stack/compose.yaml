services:
# image used to:
# scan, index, manage and serve: 
  autoheal:
    image: willfarrell/autoheal:latest
    container_name: "${autoheal.container_name:-autoheal}"
    env_file:
      .env
    deploy:
      replicas: 1
    environment:
      AUTOHEAL_CONTAINER_LABEL: "${autoheal.environment.AUTOHEAL_CONTAINER_LABEL:-keep_healthy}"
    network_mode: "${autoheal.container_network_mode:-none}"
    restart: "${autoheal_restart:-always}"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/var/run/docker.sock
# image used to:
# scan, index, manage and serve: 
  uptime-kuma:
    image: louislam/uptime-kuma:latest
    container_name: uptime-kuma
    env_file:
      .env
    restart: always
    ports:
      - "3002:3002" # This maps the container port "3001" to the host port "3001"
    volumes:
      - ./uptime-kuma/data:/app/data # Configuring persistent storage
    environment:
      - TZ=Europe/Amsterdam # Set the timezone (change to your preferred local timezone so monitoring times are the same)
      - UMASK=0022 # Set your file permissions manually
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 5s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
# image used to:
# scan, index, manage and serve: 
  netdata:
    image: netdata/netdata:edge
    container_name: netdata
    pid: host
    network_mode: host
    restart: unless-stopped
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    volumes:
      - netdataconfig:/etc/netdata
      - netdatalib:/var/lib/netdata
      - netdatacache:/var/cache/netdata
      - /:/host/root:ro,rslave
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /etc/localtime:/etc/localtime:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
      - /var/log:/host/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /run/dbus:/run/dbus:ro
    environment:
      - NETDATA_CLAIM_TOKEN=3a_tcqnbrp_Dj5VoIvy5pMa3qqdZlxwhV6_x-Armwgw7HVY3DNIDXm9Lm6ZeNbnpmki-IagEPKi-R1_JybqgOQj95QaqNZzh-xc3pc3B0FjfLCmTPZRofZLUnrcUmrEgVhUe26o
      - NETDATA_CLAIM_URL=https://app.netdata.cloud
      - NETDATA_CLAIM_ROOMS=1a9352b6-d34d-4688-bab5-06a719586033
# image used to:
# scan, index, manage and serve: 
  n8n:
    image: docker.n8n.io/n8nio/n8n
    restart: always
    container_name: n8n
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    ports:
      - "127.0.0.1:5678:5678"
    environment:
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
      - N8N_HOST=${SUBDOMAIN}.${DOMAIN_NAME}
      - N8N_PORT=5678
      - N8N_PROTOCOL=https
      - N8N_RUNNERS_ENABLED=true
      - NODE_ENV=production
      - WEBHOOK_URL=https://${SUBDOMAIN}.${DOMAIN_NAME}/
      - GENERIC_TIMEZONE=${GENERIC_TIMEZONE}
      - TZ=${GENERIC_TIMEZONE}
    volumes:
      - n8n_data:/home/node/.n8n
      - ./n8n/local-files:/files
# image used to:
# scan, index, manage and serve: 
  dashy:
    image: lissy93/dashy
    # To build from source, replace 'image: lissy93/dashy' with 'build: .'
    # build: .
    container_name: Dashy
    # Pass in your config file below, by specifying the path on your host machine
    # volumes:
    # - ./dashy/config.yml:/app/user-data/conf.yml
    restart: unless-stopped
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    ports:
      - 8383:8080
    # Set any environmental variables
    environment:
      - NODE_ENV=production
    # Specify your user ID and group ID. You can find this by running `id -u` and `id -g`
    #  - UID=1000
    #  - GID=1000
    # Specify restart policy
    # Configure healthchecks
    healthcheck:
      test: ["CMD", "node", "/app/services/healthcheck"]
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      autoheal-app: true
# image used to:
# scan, index, manage and serve: 
  dockge:
    container_name: dockge
    image: louislam/dockge:1
    hostname: dockge
    restart: unless-stopped
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    ports:
      - 5001:5001
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dockge/data:/app/data
      # Stacks Directory
      # ⚠️ READ IT CAREFULLY. If you did it wrong, your data could end up writing into a WRONG PATH.
      # ⚠️ 1. FULL path only. No relative path (MUST)
      # ⚠️ 2. Left Stacks Path === Right Stacks Path (MUST)
      - /media/rizzo/RAIDSTATION/stacks/management-stack/dockge:/media/rizzo/RAIDSTATION/stacks/management-stack/dockge
    environment:
      # Tell Dockge where to find the stacks
      - DOCKGE_STACKS_DIR=/media/rizzo/RAIDSTATION/stacks
    labels:
      autoheal-app: true
# image used to:
# scan, index, manage and serve: 
  portainer:
    container_name: portainer
    hostname: portainer
    restart: unless-stopped
    image: portainer/portainer-ce:latest
    ports:
      - 8000:8000
      - 9443:9443
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    volumes:
      - ./portainer/portainer_data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    labels:
      autoheal-app: true
# image used to:
# scan, index, manage and serve: 
  nginx-proxy-manager:
    image: "jc21/nginx-proxy-manager:latest"
    restart: unless-stopped
    hostname: nginx-proxy-manager
    container_name: nginx-proxy-manager
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    ports:
      # These ports are in format <host-port>:<container-port>
      - "80:80" # Public HTTP Port
      - "443:443" # Public HTTPS Port
      - "81:81" # Admin Web Port
      # Add any other Stream port you want to expose
      # - "21:21" # FTP
    # environment:
    #   INITIAL_ADMIN_EMAIL: ${NGINX_PROXY_MANAGER_USERNAME:-bauke.molenaar@gmail.com}
    #   INITIAL_ADMIN_PASSWORD: ${NGINX_PROXY_MANAGER_PASSWORD:-changeme}
    # Uncomment this if you want to change the location of
    # the SQLite DB file within the container
    # DB_SQLITE_FILE: "/data/database.sqlite"
    # Uncomment this if IPv6 is not enabled on your host
    # DISABLE_IPV6: 'true'
    volumes:
      - ./nginx-proxy-manager/data:/data
      - ./nginx-proxy-manager/letsencrypt:/etc/letsencrypt
    labels:
      autoheal-app: true
# image used to:
# scan, index, manage and serve: 
  grafana:
    image: grafana/grafana
    container_name: grafana
    restart: unless-stopped
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    environment:
      - GF_SECURITY_ADMIN_USER=${grafana_admin}
      - GF_SECURITY_ADMIN_PASSWORD=${grafana_password} #change the password
      - GF_INSTALL_PLUGINS=
    ports:
      - "3004:3004"
    volumes:
      - grafana_data:/var/lib/grafana
    labels:
      autoheal-app: true
# image used to:
# scan, index, manage and serve: 
  docker-proxy:
    image: tecnativa/docker-socket-proxy:latest
    container_name: portracker-docker-proxy
    restart: unless-stopped
    environment:
      - CONTAINERS=1
      - IMAGES=1
      - INFO=1
      - NETWORKS=1
      - POST=0
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "2375:2375"
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    labels:
      autoheal-app: true
# image used to:
# scan, index, manage and serve: 
  portracker:
    image: mostafawahied/portracker:latest
    container_name: portracker
    restart: unless-stopped
    pid: "host"
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    volumes:
      - ./portracker/data:/data
    ports:
      - "4999:4999"
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    environment:
      - DOCKER_HOST=tcp://docker-proxy:2375
    depends_on:
      - docker-proxy
    labels:
      autoheal-app: true
# image used to:
# scan, index, manage and serve: 
  # postgres:
  #   image: ${ZABBIX_POSTGRES_IMAGE_TAG}
  #   volumes:
  #     - zabbix-postgres:/var/lib/postgresql/data
  #   environment:
  #     POSTGRES_DB: ${ZABBIX_DB_NAME}
  #     POSTGRES_USER: ${ZABBIX_DB_USER}
  #     POSTGRES_PASSWORD: ${ZABBIX_DB_PASSWORD}
  #   networks:
  #     - zabbix-network
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "pg_isready",
  #         "-q",
  #         "-d",
  #         "${ZABBIX_DB_NAME}",
  #         "-U",
  #         "${ZABBIX_DB_USER}",
  #       ]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #     start_period: 60s
  #   restart: unless-stopped

  # zabbix-server:
  #   image: ${ZABBIX_SERVER_IMAGE_TAG}
  #   environment:
  #     DB_SERVER_HOST: postgres
  #     DB_SERVER_PORT: 5432
  #     POSTGRES_DB: ${ZABBIX_DB_NAME}
  #     POSTGRES_USER: ${ZABBIX_DB_USER}
  #     POSTGRES_PASSWORD: ${ZABBIX_DB_PASSWORD}
  #     ZBX_CACHESIZE: ${ZABBIX_CACHESIZE}
  #   networks:
  #     - zabbix-network
  #   ports:
  #     - "10051:10051"
  #   healthcheck:
  #     test: grep -qr "zabbix_server" /proc/*/status || exit 1
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #     start_period: 90s
  #   restart: unless-stopped
  #   depends_on:
  #     postgres:
  #       condition: service_healthy

  # zabbix-dashboard:
  #   image: ${ZABBIX_WEB_IMAGE_TAG}
  #   environment:
  #     DB_SERVER_HOST: postgres
  #     DB_SERVER_PORT: 5432
  #     POSTGRES_DB: ${ZABBIX_DB_NAME}
  #     POSTGRES_USER: ${ZABBIX_DB_USER}
  #     POSTGRES_PASSWORD: ${ZABBIX_DB_PASSWORD}
  #     ZBX_SERVER_HOST: zabbix-server
  #     PHP_TZ: ${ZABBIX_TIMEZONE}
  #   networks:
  #     - zabbix-network
  #   ports:
  #     - "80:8080"
  #     - "443:8443"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #     start_period: 90s
  #   restart: unless-stopped
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     zabbix-server:
  #       condition: service_healthy

  # zabbix-agent:
  #   image: ${ZABBIX_AGENT_IMAGE_TAG}
  #   environment:
  #     ZBX_HOSTNAME: Zabbix server
  #     ZBX_SERVER_HOST: zabbix-server
  #   networks:
  #     - zabbix-network
  #   restart: unless-stopped
  #   depends_on:
  #     - postgres
  #     - zabbix-server

  # backups:
  #   image: ${ZABBIX_POSTGRES_IMAGE_TAG}
  #   command: >-
  #     sh -c 'sleep $ZABBIX_BACKUP_INIT_SLEEP &&
  #     while true; do
  #       pg_dump -h postgres -p 5432 -d $ZABBIX_DB_NAME -U $ZABBIX_DB_USER | gzip > $ZABBIX_POSTGRES_BACKUPS_PATH/$ZABBIX_POSTGRES_BACKUP_NAME-$(date "+%Y-%m-%d_%H-%M").gz &&
  #       find $ZABBIX_POSTGRES_BACKUPS_PATH -type f -mtime +$ZABBIX_POSTGRES_BACKUP_PRUNE_DAYS | xargs rm -f &&
  #       sleep $ZABBIX_BACKUP_INTERVAL; done'
  #   volumes:
  #     - zabbix-postgres-backup:/var/lib/postgresql/data
  #     - zabbix-database-backups:${ZABBIX_POSTGRES_BACKUPS_PATH}
  #   environment:
  #     ZABBIX_DB_NAME: ${ZABBIX_DB_NAME}
  #     ZABBIX_DB_USER: ${ZABBIX_DB_USER}
  #     PGPASSWORD: ${ZABBIX_DB_PASSWORD}
  #     ZABBIX_BACKUP_INIT_SLEEP: ${ZABBIX_BACKUP_INIT_SLEEP}
  #     ZABBIX_BACKUP_INTERVAL: ${ZABBIX_BACKUP_INTERVAL}
  #     ZABBIX_POSTGRES_BACKUP_PRUNE_DAYS: ${ZABBIX_POSTGRES_BACKUP_PRUNE_DAYS}
  #     ZABBIX_POSTGRES_BACKUPS_PATH: ${ZABBIX_POSTGRES_BACKUPS_PATH}
  #     ZABBIX_POSTGRES_BACKUP_NAME: ${ZABBIX_POSTGRES_BACKUP_NAME}
  #   networks:
  #     - zabbix-network
  #   restart: unless-stopped
  #   depends_on:
  #     postgres:
  #       condition: service_healthy

# image used to:
# scan, index, manage and serve: 
  nextcloud:
    image: nextcloud
    container_name: nextcloud
    restart: unless-stopped
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    depends_on:
      - nextclouddb
      - redis
    ports:
      - 8087:80
    volumes:
      - ./nextcloud/html:/var/www/html
      - ./nextcloud/custom_apps:/var/www/html/custom_apps
      - ./nextcloud/config:/var/www/html/config
      - ./nextcloud/data:/var/www/html/data
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Amsterdam
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
      - MYSQL_PASSWORD=dbpassword
      - MYSQL_HOST=nextclouddb
      - REDIS_HOST=redis
# image used to:
# scan, index, manage and serve: 
  nextclouddb:
    image: mariadb
    container_name: nextcloud-db
    restart: unless-stopped
    command: --transaction-isolation=READ-COMMITTED --binlog-format=ROW
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    volumes:
      - ./nextclouddb/nextclouddb:/var/lib/mysql
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Amsterdam
      - MYSQL_RANDOM_ROOT_PASSWORD=true
      - MYSQL_PASSWORD=dbpassword
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
# image used to:
# scan, index, manage and serve: 
  collabora:
    image: collabora/code
    container_name: collabora
    restart: unless-stopped
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Amsterdam
      - password=password
      - username=nextcloud
      - domain=example.com
      - extra_params=--o:ssl.enable=true
    ports:
      - 9980:9980
# image used to:
# scan, index, manage and serve: 
  redis:
    image: redis:alpine
    container_name: redis
    volumes:
      - ./redis/redis:/data
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
# image used to:
# scan, index, manage and serve: 
  wg-easy:
    #environment:
    #  Optional:
    #  - PORT=51821
    #  - HOST=0.0.0.0
    #  - INSECURE=false

    image: ghcr.io/wg-easy/wg-easy:15
    container_name: wg-easy
    networks:
      wg:
        ipv4_address: 10.42.42.42
        ipv6_address: fdcc:ad94:bacf:61a3::2a
    volumes:
      - etc_wireguard:/etc/wireguard
      - /lib/modules:/lib/modules:ro
    ports:
      - "51820:51820/udp"
      - "51821:51821/tcp"
    restart: unless-stopped
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
      # - NET_RAW # ⚠️ Uncomment if using Podman
    sysctls:
      - net.ipv4.ip_forward=1
      - net.ipv4.conf.all.src_valid_mark=1
      - net.ipv6.conf.all.disable_ipv6=0
      - net.ipv6.conf.all.forwarding=1
      - net.ipv6.conf.default.forwarding=1
# image used to:
# scan, index, manage and serve: 
  watchtower:
    image: "containrrr/watchtower:latest"
    restart: unless-stopped
    container_name: watchtower-management-stack
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    # command: --debug --http-api-update
    # environment:
    #   - WATCHTOWER_HTTP_API_TOKEN=${WATCHTOWER_HTTP_API_TOKEN}
    networks:
      - ai-stack
      - management-stack
      - kuma_network
      - cloud
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
      - "autoheal-app=true"
    depends_on:
      - autoheal
      - collabora
      - dashy
      - docker-proxy
      - dockge
      - grafana
      - n8n
      - netdata
      - nextcloud
      - nextclouddb
      - nginx-proxy-manager
      - portainer
      - portracker
      - redis
      - uptime-kuma
      - wg-easy
      # - watchtower


volumes:
  data:
  grafana_data:
  zabbix-postgres:
  zabbix-postgres-backup:
  zabbix-database-backups:
  n8n_data:
  traefik_data:
  netdataconfig:
  netdatalib:
  netdatacache:
  etc_wireguard:

networks:
  ai-stack:
    external: true
  iot_macvlan:
    external: true
  basic-memory-net:
  management-stack:
    external: true
  proxy:
    external: true
  cloud:
    name: cloud
    driver: bridge
    external: true
  zabbix-network:
    external: true
  kuma_network:
    driver: bridge
    external: true
  wg:
    driver: bridge
    enable_ipv6: true
    ipam:
      driver: default
      config:
        - subnet: 10.42.42.0/24
        - subnet: fdcc:ad94:bacf:61a3::/64
