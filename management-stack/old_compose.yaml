services:
  #
  # MANAGEMENT STACK - DOCKER COMPOSE
  #
  # Best Practices:
  # - Use variable substitution for all sensitive/configurable values
  # - Use Docker secrets for passwords/keys
  # - Pin image versions
  # - Add healthchecks for all services
  # - Use resource limits (mem_limit, cpus) where possible
  # - Segment networks for security
  # - Add comments for clarity
  # - Plan for backups and CI/CD
  # - Document all customizations
  #
  # scan, index, manage and serve:
  autoheal:
    # TODO: Add resource limits if supported by image
    image: ${AUTOHEAL_IMAGE:-willfarrell/autoheal:latest}
    container_name: ${AUTOHEAL_CONTAINER_NAME:-autoheal}
    env_file:
      - .env
    deploy:
      replicas: 1
    environment:
      AUTOHEAL_CONTAINER_LABEL: ${AUTOHEAL_CONTAINER_LABEL:-keep_healthy}
    # network_mode: ${AUTOHEAL_NETWORK_MODE:-none}
    restart: ${AUTOHEAL_RESTART:-always}
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /var/run/docker.sock:/var/run/docker.sock
  # image used to:
  # scan, index, manage and serve:
  uptime-kuma:
    # TODO: Add resource limits if supported by image
    image: ${UPTIME_KUMA_IMAGE:-louislam/uptime-kuma:latest}
    container_name: ${uptime_kuma_container_name:-uptime-kuma}
    env_file:
      - .env
    restart: ${uptime_kuma_restart:-always}
    ports:
      - "${UPTIME_KUMA_PORT:-3002}:3002"
    volumes:
      - ./uptime-kuma/data:/app/data
    environment:
      - TZ=${UPTIME_KUMA_TZ:-Europe/Amsterdam}
      - UMASK=${UPTIME_KUMA_UMASK:-0022}
    networks:
      - management-stack
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 5s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
  # image used to:
  # scan, index, manage and serve:
  netdata:
    # TODO: Add resource limits if supported by image
    image: ${NETDATA_IMAGE:-netdata/netdata:edge}
    container_name: ${netdata_container_name:-netdata}
    pid: host
    network_mode: host
    restart: ${netdata_restart:-unless-stopped}
    ports:
      - "${NETDATA_PORT:-19999}:19999"
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    volumes:
      - netdataconfig:/etc/netdata
      - netdatalib:/var/lib/netdata
      - netdatacache:/var/cache/netdata
      - /:/host/root:ro,rslave
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /etc/localtime:/etc/localtime:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
      - /var/log:/host/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /run/dbus:/run/dbus:ro
    environment:
      - NETDATA_CLAIM_TOKEN=3a_tcqnbrp_Dj5VoIvy5pMa3qqdZlxwhV6_x-Armwgw7HVY3DNIDXm9Lm6ZeNbnpmki-IagEPKi-R1_JybqgOQj95QaqNZzh-xc3pc3B0FjfLCmTPZRofZLUnrcUmrEgVhUe26o
      - NETDATA_CLAIM_URL=https://app.netdata.cloud
      - NETDATA_CLAIM_ROOMS=1a9352b6-d34d-4688-bab5-06a719586033
  # image used to:
  # scan, index, manage and serve:
  n8n:
    # TODO: Add resource limits if supported by image
    image: ${N8N_IMAGE:-docker.n8n.io/n8nio/n8n}
    restart: ${n8n_restart:-always}
    container_name: ${n8n_container_name:-n8n}
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    ports:
      - "127.0.0.1:5678:5678"
    environment:
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
      - N8N_HOST=${SUBDOMAIN:-n8n}.${DOMAIN_NAME:-example.com}
      - N8N_PORT=5678
      - N8N_PROTOCOL=https
      - N8N_RUNNERS_ENABLED=true
      - NODE_ENV=production
      - WEBHOOK_URL=https://${SUBDOMAIN:-n8n}.${DOMAIN_NAME:-example.com}/
      - GENERIC_TIMEZONE=${GENERIC_TIMEZONE:-Europe/Amsterdam}
      - TZ=${GENERIC_TIMEZONE:-Europe/Amsterdam}
    volumes:
      - n8n_data:/home/node/.n8n
      - ./n8n/local-files:/files
  # image used to:
  # scan, index, manage and serve:
  dashy:
    # TODO: Add resource limits if supported by image
    image: ${DASHY_IMAGE:-lissy93/dashy}
    # To build from source, replace 'image: lissy93/dashy' with 'build: .'
    # build: .
    container_name: ${dashy_container_name:-Dashy}
    # Pass in your config file below, by specifying the path on your host machine
    # volumes:
    #   - ./dashy/config.yml:/app/user-data/conf.yml
    restart: ${dashy_restart:-unless-stopped}
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    ports:
      - 8383:8080
    # Set any environmental variables
    environment:
      - NODE_ENV=production
    # Specify your user ID and group ID. You can find this by running `id -u` and `id -g`
    #   - UID=1000
    #   - GID=1000
    # Specify restart policy
    # Configure healthchecks
    healthcheck:
      test: ["CMD", "node", "/app/services/healthcheck"]
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 40s
    labels:
      autoheal-app: true
  # image used to:
  # scan, index, manage and serve:
  dockge:
    # TODO: Add resource limits if supported by image
    container_name: ${dockge_container_name:-dockge}
    image: ${DOCKGE_IMAGE:-louislam/dockge:1}
    hostname: ${dockge_hostname:-dockge}
    restart: ${dockge_restart:-unless-stopped}
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    ports:
      - 5001:5001
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./dockge/data:/app/data
      # Stacks Directory
      # ⚠️ READ IT CAREFULLY. If you did it wrong, your data could end up writing into a WRONG PATH.
      # ⚠️ 1. FULL path only. No relative path (MUST)
      # ⚠️ 2. Left Stacks Path === Right Stacks Path (MUST)
      - /media/rizzo/RAIDSTATION/stacks/management-stack/dockge:/media/rizzo/RAIDSTATION/stacks/management-stack/dockge
    environment:
      # Tell Dockge where to find the stacks
      - DOCKGE_STACKS_DIR=/media/rizzo/RAIDSTATION/stacks
    labels:
      autoheal-app: true
  # image used to:
  # scan, index, manage and serve:
  portainer:
    # TODO: Add resource limits if supported by image
    #
    # BACKUP STRATEGY:
    # - Ensure all named volumes are included in regular backups
    # - Use offsite/cloud backup for critical data
    # - Document restore procedures
    #
    # SECRETS MANAGEMENT:
    # - Store all secret files securely
    # - Rotate secrets regularly
    # - Never commit secrets to version control
    #
    # NETWORK SEGMENTATION:
    # - Use dedicated networks for each stack
    # - Restrict inter-stack communication unless required
    # - Document network policies
    #
    # CI/CD & DOCUMENTATION:
    # - Use GitHub Actions or similar for automated testing/deployment
    # - Document all customizations and manual steps in README.md
    # - Keep this file and .env in sync
    container_name: ${portainer_container_name:-portainer}
    hostname: ${portainer_hostname:-portainer}
    restart: ${portainer_restart:-unless-stopped}
    image: ${PORTAINER_IMAGE:-portainer/portainer-ce:latest}
    ports:
      - 8000:8000
      - 9443:9443
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    volumes:
      - ./portainer/portainer_data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    labels:
      autoheal-app: true
  # image used to:
  # scan, index, manage and serve:
  nginx-proxy-manager:
    image: ${NGINX_PROXY_MANAGER_IMAGE:-jc21/nginx-proxy-manager:latest}
    restart: ${nginx_proxy_manager_restart:-unless-stopped}
    hostname: ${nginx_proxy_manager_hostname:-nginx-proxy-manager}
    container_name: ${nginx_proxy_manager_container_name:-nginx-proxy-manager}
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    ports:
      # These ports are in format <host-port>:<container-port>
      - "80:80" # Public HTTP Port
      - "443:443" # Public HTTPS Port
      - "81:81" # Admin Web Port
      # Add any other Stream port you want to expose
      # - "21:21" # FTP
    # environment:
    #   INITIAL_ADMIN_EMAIL: ${NGINX_PROXY_MANAGER_USERNAME:-bauke.molenaar@gmail.com}
    #   INITIAL_ADMIN_PASSWORD: ${NGINX_PROXY_MANAGER_PASSWORD:-changeme}
    # Uncomment this if you want to change the location of
    # the SQLite DB file within the container
    # DB_SQLITE_FILE: "/data/database.sqlite"
    # Uncomment this if IPv6 is not enabled on your host
    # DISABLE_IPV6: 'true'
    volumes:
      - ./nginx-proxy-manager/data:/data
      - ./nginx-proxy-manager/letsencrypt:/etc/letsencrypt
    labels:
      autoheal-app: true
  # image used to:
  # scan, index, manage and serve:
  grafana:
    image: ${GRAFANA_IMAGE:-grafana/grafana}
    container_name: ${grafana_container_name:-grafana}
    restart: ${grafana_restart:-unless-stopped}
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    environment:
      - GF_SECURITY_ADMIN_USER=${grafana_admin}
      - GF_SECURITY_ADMIN_PASSWORD=${grafana_password}
      - GF_INSTALL_PLUGINS=
    ports:
      - "3004:3004"
    volumes:
      - grafana_data:/var/lib/grafana
    labels:
      autoheal-app: true
  # image used to:
  # scan, index, manage and serve:
  docker-proxy:
    image: ${DOCKER_PROXY_IMAGE:-tecnativa/docker-socket-proxy:latest}
    container_name: ${docker_proxy_container_name:-portracker-docker-proxy}
    restart: ${docker_proxy_restart:-unless-stopped}
    environment:
      - CONTAINERS=1
      - IMAGES=1
      - INFO=1
      - NETWORKS=1
      - POST=0
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "2375:2375"
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    labels:
      autoheal-app: true
  # image used to:
  # scan, index, manage and serve:
  portracker:
    image: ${PORTRACKER_IMAGE:-mostafawahied/portracker:latest}
    container_name: ${portracker_container_name:-portracker}
    restart: ${portracker_restart:-unless-stopped}
    pid: "host"
    cap_add:
      - SYS_PTRACE
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    volumes:
      - ./portracker/data:/data
    ports:
      - "4999:4999"
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    environment:
      - DOCKER_HOST=tcp://docker-proxy:2375
    depends_on:
      - docker-proxy
    labels:
      autoheal-app: true
  # image used to:
  # scan, index, manage and serve:
  # postgres:
  #   image: ${ZABBIX_POSTGRES_IMAGE_TAG}
  #   volumes:
  #     - zabbix-postgres:/var/lib/postgresql/data
  #   environment:
  #     POSTGRES_DB: ${ZABBIX_DB_NAME}
  #     POSTGRES_USER: ${ZABBIX_DB_USER}
  #     POSTGRES_PASSWORD: ${ZABBIX_DB_PASSWORD}
  #   networks:
  #     - zabbix-network
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "pg_isready",
  #         "-q",
  #         "-d",
  #         "${ZABBIX_DB_NAME}",
  #         "-U",
  #         "${ZABBIX_DB_USER}",
  #       ]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #     start_period: 60s
  #   restart: unless-stopped

  # zabbix-server:
  #   image: ${ZABBIX_SERVER_IMAGE_TAG}
  #   environment:
  #     DB_SERVER_HOST: postgres
  #     DB_SERVER_PORT: 5432
  #     POSTGRES_DB: ${ZABBIX_DB_NAME}
  #     POSTGRES_USER: ${ZABBIX_DB_USER}
  #     POSTGRES_PASSWORD: ${ZABBIX_DB_PASSWORD}
  #     ZBX_CACHESIZE: ${ZABBIX_CACHESIZE}
  #   networks:
  #     - zabbix-network
  #   ports:
  #     - "10051:10051"
  #   healthcheck:
  #     test: grep -qr "zabbix_server" /proc/*/status || exit 1
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #     start_period: 90s
  #   restart: unless-stopped
  #   depends_on:
  #     postgres:
  #       condition: service_healthy

  # zabbix-dashboard:
  #   image: ${ZABBIX_WEB_IMAGE_TAG}
  #   environment:
  #     DB_SERVER_HOST: postgres
  #     DB_SERVER_PORT: 5432
  #     POSTGRES_DB: ${ZABBIX_DB_NAME}
  #     POSTGRES_USER: ${ZABBIX_DB_USER}
  #     POSTGRES_PASSWORD: ${ZABBIX_DB_PASSWORD}
  #     ZBX_SERVER_HOST: zabbix-server
  #     PHP_TZ: ${ZABBIX_TIMEZONE}
  #   networks:
  #     - zabbix-network
  #   ports:
  #     - "80:8080"
  #     - "443:8443"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 3
  #     start_period: 90s
  #   restart: unless-stopped
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     zabbix-server:
  #       condition: service_healthy

  # zabbix-agent:
  #   image: ${ZABBIX_AGENT_IMAGE_TAG}
  #   environment:
  #     ZBX_HOSTNAME: Zabbix server
  #     ZBX_SERVER_HOST: zabbix-server
  #   networks:
  #     - zabbix-network
  #   restart: unless-stopped
  #   depends_on:
  #     - postgres
  #     - zabbix-server

  # backups:
  #   image: ${ZABBIX_POSTGRES_IMAGE_TAG}
  #   command: >-
  #     sh -c 'sleep $ZABBIX_BACKUP_INIT_SLEEP &&
  #     while true; do
  #       pg_dump -h postgres -p 5432 -d $ZABBIX_DB_NAME -U $ZABBIX_DB_USER | gzip > $ZABBIX_POSTGRES_BACKUPS_PATH/$ZABBIX_POSTGRES_BACKUP_NAME-$(date "+%Y-%m-%d_%H-%M").gz &&
  #       find $ZABBIX_POSTGRES_BACKUPS_PATH -type f -mtime +$ZABBIX_POSTGRES_BACKUP_PRUNE_DAYS | xargs rm -f &&
  #       sleep $ZABBIX_BACKUP_INTERVAL; done'
  #   volumes:
  #     - zabbix-postgres-backup:/var/lib/postgresql/data
  #     - zabbix-database-backups:${ZABBIX_POSTGRES_BACKUPS_PATH}
  #   environment:
  #     ZABBIX_DB_NAME: ${ZABBIX_DB_NAME}
  #     ZABBIX_DB_USER: ${ZABBIX_DB_USER}
  #     PGPASSWORD: ${ZABBIX_DB_PASSWORD}
  #     ZABBIX_BACKUP_INIT_SLEEP: ${ZABBIX_BACKUP_INIT_SLEEP}
  #     ZABBIX_BACKUP_INTERVAL: ${ZABBIX_BACKUP_INTERVAL}
  #     ZABBIX_POSTGRES_BACKUP_PRUNE_DAYS: ${ZABBIX_POSTGRES_BACKUP_PRUNE_DAYS}
  #     ZABBIX_POSTGRES_BACKUPS_PATH: ${ZABBIX_POSTGRES_BACKUPS_PATH}
  #     ZABBIX_POSTGRES_BACKUP_NAME: ${ZABBIX_POSTGRES_BACKUP_NAME}
  #   networks:
  #     - zabbix-network
  #   restart: unless-stopped
  #   depends_on:
  #     postgres:
  #       condition: service_healthy

  ubuntu-desktop-lxde-vnc:
    image: dorowu/ubuntu-desktop-lxde-vnc
    container_name: ubuntu-desktop-lxde-vnc
    restart: unless-stopped
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    ports:
      - "6080:80"
      - "5900:5900"
    environment:
      - "ALSADEV=hw:2,0"
      - "USER=${UBUNTU_USER:-doro}"
      - "PASSWORD=${UBUNTU_PASSWORD:-password}"
      - "VNC_PASSWORD=${UBUNTU_VNC_PASSWORD:-vncpassword}"
    volumes:
      - "/dev/shm:/dev/shm"
    devices:
      - "/dev/snd"

  # image used to:
  # scan, index, manage and serve:
  nextcloud:
    image: ${NEXTCLOUD_IMAGE:-nextcloud}
    container_name: ${nextcloud_container_name:-nextcloud}
    restart: ${nextcloud_restart:-unless-stopped}
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    depends_on:
      - nextclouddb
      - redis
    ports:
      - 8087:80
    volumes:
      - ./nextcloud/html:/var/www/html
      - ./nextcloud/custom_apps:/var/www/html/custom_apps
      - ./nextcloud/config:/var/www/html/config
      - ./nextcloud/data:/var/www/html/data
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Amsterdam
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
      - MYSQL_PASSWORD=dbpassword
      - MYSQL_HOST=nextclouddb
      - REDIS_HOST=redis
  # image used to:
  # scan, index, manage and serve:
  nextclouddb:
    image: ${NEXTCLOUDDB_IMAGE:-mariadb}
    container_name: ${nextclouddb_container_name:-nextcloud-db}
    restart: ${nextclouddb_restart:-unless-stopped}
    command: --transaction-isolation=READ-COMMITTED --binlog-format=ROW
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    volumes:
      - ./nextclouddb/nextclouddb:/var/lib/mysql
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Amsterdam
      - MYSQL_RANDOM_ROOT_PASSWORD=true
      - MYSQL_PASSWORD=dbpassword
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=nextcloud
  # image used to:
  # scan, index, manage and serve:
  collabora:
    image: ${COLLABORA_IMAGE:-collabora/code}
    container_name: ${collabora_container_name:-collabora}
    restart: ${collabora_restart:-unless-stopped}
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Amsterdam
      - password=password
      - username=nextcloud
      - domain=example.com
      - extra_params=--o:ssl.enable=true
    ports:
      - 9980:9980
  # image used to:
  # scan, index, manage and serve:
  redis:
    image: ${REDIS_IMAGE:-redis:alpine}
    container_name: ${redis_container_name:-redis}
    volumes:
      - ./redis/redis:/data
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
  # image used to:
  # scan, index, manage and serve:
  wg-easy:
    #environment:
    #  Optional:
    #  - PORT=51821
    #  - HOST=0.0.0.0
    #  - INSECURE=false

    image: ${WG_EASY_IMAGE:-ghcr.io/wg-easy/wg-easy:15}
    container_name: ${wg_easy_container_name:-wg-easy}
    networks:
      wg:
        ipv4_address: 10.42.42.42
        ipv6_address: fdcc:ad94:bacf:61a3::2a
    volumes:
      - etc_wireguard:/etc/wireguard
      - /lib/modules:/lib/modules:ro
    ports:
      - "51820:51820/udp"
      - "51821:51821/tcp"
    restart: unless-stopped
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
      # - NET_RAW # ⚠️ Uncomment if using Podman
    sysctls:
      - net.ipv4.ip_forward=1
      - net.ipv4.conf.all.src_valid_mark=1
      - net.ipv6.conf.all.disable_ipv6=0
      - net.ipv6.conf.all.forwarding=1
      - net.ipv6.conf.default.forwarding=1
  # image used to:
  # scan, index, manage and serve:
  watchtower:
    image: ${WATCHTOWER_IMAGE:-containrrr/watchtower:latest}
    restart: ${watchtower_restart:-unless-stopped}
    container_name: ${watchtower_container_name:-watchtower-management-stack}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    # command: --debug --http-api-update
    # environment:
    #   - WATCHTOWER_HTTP_API_TOKEN=${WATCHTOWER_HTTP_API_TOKEN}
    networks:
      - ai-services
      - management-stack
      - kuma_network
      - cloud
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
      - "autoheal-app=true"
    depends_on:
      - autoheal
      - collabora
      - dashy
      - docker-proxy
      - dockge
      - grafana
      - n8n
      - netdata
      - nextcloud
      - nextclouddb
      - nginx-proxy-manager
      - portainer
      - portracker
      - redis
      - uptime-kuma
      - wg-easy
      # - watchtower

volumes:
  data:
  grafana_data:
  zabbix-postgres:
  zabbix-postgres-backup:
  zabbix-database-backups:
  n8n_data:
  traefik_data:
  netdataconfig:
  netdatalib:
  netdatacache:
  etc_wireguard:

networks:
  ai-stack:
    external: true
  iot_macvlan:
    external: true
  basic-memory-net:
  management-stack:
    external: true
  proxy:
    external: true
  cloud:
    name: cloud
    driver: bridge
    external: true
  zabbix-network:
    external: true
  kuma_network:
    driver: bridge
    external: true
  wg:
    driver: bridge
    enable_ipv6: true
    ipam:
      driver: default
      config:
        - subnet: 10.42.42.0/24
        - subnet: fdcc:ad94:bacf:61a3::/64
