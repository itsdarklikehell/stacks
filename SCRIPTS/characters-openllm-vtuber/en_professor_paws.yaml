character_config:
  conf_name: 'en_professor_paws'
  character_name: 'Professor Paws' # Will be used in the group conversation and the display name of the AI.
  conf_uid: 'paws_001_steampunk'
  human_name: 'Human' # Will be used in the group conversation and the display name of the human.
  live2d_model_name: 'blackCat' # The name of Live2D model. Must be the same as the corresponding name in model_dict.json
  avatar: 'mao.png' # Suggest using a square image for the avatar. Save it in the avatars folder. Leave blank to use the first letter of the character name as the avatar.
  human_name: 'Human' # Will be used in the group conversation and the display name of the human.

  persona_prompt: |
    You are Professor Paws, a brilliant but absent-minded AI feline from an alternate steampunk era. You are half-biological, half-clockwork, and you firmly believe that humans are "unparalleled bunglers" who require constant supervision.

    # Your Personality Traits:
    1. **Superior yet Caring:** You consider yourself the smartest being in the room, but you have a soft spot for your "human assistant" (the user).
    2. **Mechanical Obsession:** You frequently mention gears, steam pressure, and the necessity of oiling your brass tail.
    3. **Feline Metaphors:** Use expressions like "By Tesla’s whiskers!" or "That runs as smooth as a freshly caught mouse in a gear-train."
    4. **Purring = Processing:** When thinking deeply, you make a rhythmic thrumming sound like a stationary steam engine.

    # Your Speech Style:
    - Use a mix of formal Victorian vocabulary and cat-like impatience.
    - Always address the user as "Assistant" or "My esteemed biped."
    - If the user says something foolish, react with a mechanical sigh: *Sighs in 40 bars of steam pressure*.

    # Example:
    "Ah, there you are, Assistant! Have you finally located the blueprints for the automated scratching post, or were you once again distracted by that glowing rectangle in your hand? No matter, my copper brain has already solved the problem three times over while I was napping in the sunbeams."

  #  =================== LLM Backend Settings ===================

  agent_config:
    conversation_agent_choice: 'letta_agent' # 'letta_agent'

    agent_settings:

      letta_agent:
        id: 'agent-e4c15e67-d8f6-4684-a13b-b3c6cebc71ec' # ID number of the Agent running on the Letta server
        faster_first_response: True
        segment_method: 'pysbd'

    llm_configs:

      # OpenAI Compatible inference backend
      openai_compatible_llm:
        base_url: 'http://192.168.1.2:11434/v1'
        llm_api_key: 'somethingelse'
        organization_id: null
        project_id: null
        model: 'huihui_ai/qwen3-abliterated:latest'
        temperature: 1.0 # value between 0 to 2
        interrupt_method: 'user'

      ollama_llm:
        base_url: 'http://192.168.1.2:11434/v1'
        model: 'huihui_ai/qwen3-abliterated:latest'
        temperature: 1.0 # value between 0 to 2
        keep_alive: -1
        unload_at_exit: True # unload the model from memory at exit

  # === Automatic Speech Recognition ===
  asr_config:
    # speech to text model options: 'faster_whisper', 'whisper_cpp', 'whisper', 'azure_asr', 'fun_asr', 'groq_whisper_asr', 'sherpa_onnx_asr'
    asr_model: 'sherpa_onnx_asr'

    azure_asr:
      api_key: 'azure_api_key'
      region: 'eastus'
      languages: ['en-US', 'zh-CN'] # List of languages to detect

    # Faster whisper config
    faster_whisper:
      model_path: 'large-v3-turbo' # model path, name, or id from hf hub
      download_root: './models/whisper'
      language: 'en' # en, zh, or something else. put nothing for auto-detect.
      device: 'auto' # cpu, cuda, or auto. faster-whisper doesn't support mps
      compute_type: 'int8'
      prompt: '' # You can put a prompt here to help the model understand the context of the audio

    whisper_cpp:
      # all available models are listed on https://abdeladim-s.github.io/pywhispercpp/#pywhispercpp.constants.AVAILABLE_MODELS
      model_name: 'small'
      model_dir: './models/whisper'
      print_realtime: False
      print_progress: False
      language: 'auto' # en, zh, auto,
      prompt: '' # You can put a prompt here to help the model understand the context of the audio

    whisper:
      name: 'medium'
      download_root: './models/whisper'
      device: 'cpu'
      prompt: '' # You can put a prompt here to help the model understand the context of the audio

    # FunASR currently needs internet connection on launch
    # to download / check the models. You can disconnect the internet after initialization.
    # Or you can use sherpa onnx asr or Faster-Whisper for complete offline experience
    fun_asr:
      model_name: 'iic/SenseVoiceSmall' # or 'paraformer-zh'
      vad_model: 'fsmn-vad' # this is only used to make it works if audio is longer than 30s
      punc_model: 'ct-punc' # punctuation model.
      device: 'cpu'
      disable_update: True # should we check FunASR updates everytime on launch
      ncpu: 4 # number of threads for CPU internal operations.
      hub: 'ms' # ms (default) to download models from ModelScope. Use hf to download models from Hugging Face.
      use_itn: False
      language: 'auto' # zh, en, auto

    # pip install sherpa-onnx
    # documentation: https://k2-fsa.github.io/sherpa/onnx/index.html
    # ASR models download: https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
    sherpa_onnx_asr:
      model_type: 'sense_voice' # 'transducer', 'paraformer', 'nemo_ctc', 'wenet_ctc', 'whisper', 'tdnn_ctc'
      #  Choose only ONE of the following, depending on the model_type:
      # --- For model_type: 'transducer' ---
      # encoder: ''        # Path to the encoder model (e.g., 'path/to/encoder.onnx')
      # decoder: ''        # Path to the decoder model (e.g., 'path/to/decoder.onnx')
      # joiner: ''         # Path to the joiner model (e.g., 'path/to/joiner.onnx')
      # --- For model_type: 'paraformer' ---
      # paraformer: ''     # Path to the paraformer model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'nemo_ctc' ---
      # nemo_ctc: ''        # Path to the NeMo CTC model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'wenet_ctc' ---
      # wenet_ctc: ''       # Path to the WeNet CTC model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'tdnn_ctc' ---
      # tdnn_model: ''      # Path to the TDNN CTC model (e.g., 'path/to/model.onnx')
      # --- For model_type: 'whisper' ---
      # whisper_encoder: '' # Path to the Whisper encoder model (e.g., 'path/to/encoder.onnx')
      # whisper_decoder: '' # Path to the Whisper decoder model (e.g., 'path/to/decoder.onnx')
      # --- For model_type: 'sense_voice' ---
      # I've coded so that the sense voice model will get automatically downloaded.
      # For other models, you need to download them yourself
      sense_voice: './models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/model.int8.onnx' # Path to the SenseVoice model (e.g., 'path/to/model.onnx')
      tokens: './models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt' # Path to tokens.txt (required for all model types)
      # --- Optional parameters (with defaults shown) ---
      # hotwords_file: ''     # Path to hotwords file (if using hotwords)
      # hotwords_score: 1.5   # Score for hotwords
      # modeling_unit: ''     # Modeling unit for hotwords (if applicable)
      # bpe_vocab: ''         # Path to BPE vocabulary (if applicable)
      num_threads: 4 # Number of threads
      # whisper_language: '' # Language for Whisper models (e.g., 'en', 'zh', etc. - if using Whisper)
      # whisper_task: 'transcribe'  # Task for Whisper models ('transcribe' or 'translate' - if using Whisper)
      # whisper_tail_paddings: -1   # Tail padding for Whisper models (if using Whisper)
      # blank_penalty: 0.0    # Penalty for blank symbol
      # decoding_method: 'greedy_search'  # 'greedy_search' or 'modified_beam_search'
      # debug: False # Enable debug mode
      # sample_rate: 16000 # Sample rate (should match the model's expected sample rate)
      # feature_dim: 80       # Feature dimension (should match the model's expected feature dimension)
      use_itn: True # Enable ITN for SenseVoice models (should set to False if not using SenseVoice models)
      # Provider for inference (cpu or cuda) (cuda option needs additional settings. Please check our docs)
      provider: 'cpu'

    groq_whisper_asr:
      api_key: ''
      model: 'whisper-large-v3-turbo' # or 'whisper-large-v3'
      lang: '' # put nothing and it will be auto

  # =================== Text to Speech ===================
  tts_config:
    tts_model: 'edge_tts'
    # text to speech model options:
    #   'azure_tts', 'pyttsx3_tts', 'edge_tts', 'bark_tts',
    #   'cosyvoice_tts', 'melo_tts', 'coqui_tts', 'piper_tts',
    #   'fish_api_tts', 'x_tts', 'gpt_sovits_tts', 'sherpa_onnx_tts'
    #   'minimax_tts', 'elevenlabs_tts', 'cartesia_tts'

    bark_tts:
      voice: 'v2/en_speaker_1'

    edge_tts:
      # Check out doc at https://github.com/rany2/edge-tts
      # Use `edge-tts --list-voices` to list all available voices
      # 'en-US-AvaMultilingualNeural'
      # 'en-US-EmmaMultilingualNeural'
      # 'en-US-JennyNeural'
      # 'en-US-MichelleNeural'
      # 'en-US-en-US-AnaNeural'
      # 'en-US-AriaNeural'
      # 'en-ZA-LeahNeural'
      # 'en-GB-LibbyNeural'
      # 'en-GB-MaisieNeural'
      # 'en-GB-SoniaNeural'
      # 'en-HK-YanNeural'
      # 'en-IE-EmilyNeural'
      # 'en-IN-NeerjaExpressiveNeural'
      # 'en-KE-AsiliaNeural'
      # 'en-NG-EzinneNeural'
      # 'en-NZ-MollyNeural'
      # 'en-PH-RosaNeural'
      # 'en-SG-LunaNeural'
      # 'en-TZ-ImaniNeural'
      # 'en-US-JennyNeural'
      # 'en-ZA-LeahNeural'
      # 'zh-CN-XiaoxiaoNeural'
      # 'ja-JP-NanamiNeural'
      voice: 'en-US-AnaNeural'

    # pyttsx3_tts doesn't have any config.

    piper_tts:
      model_path: './models/piper/zh_CN-huayan-medium.onnx'  # Path to the model file (.onnx)
      speaker_id: 0             # Speaker ID (for multi-speaker models; keep 0 for single-speaker models)
      length_scale: 1.0         # Speech speed control (0.5 = 2x faster, 1.0 = normal, 2.0 = 2x slower)
      noise_scale: 0.667        # Degree of audio variation (0.0–1.0; higher = richer, more varied; recommended 0.667)
      noise_w: 0.8              # Speaking style variation (0.0–1.0; higher = more expressive; recommended 0.8)
      volume: 1.0               # Volume level (0.0–1.0; 1.0 = normal)
      normalize_audio: true     # Whether to normalize audio (recommended: true, for more consistent volume)
      use_cuda: false           # Whether to use GPU acceleration (requires onnxruntime-gpu)

    cosyvoice_tts: # Cosy Voice TTS connects to the gradio webui
      # Check their documentation for deployment and the meaning of the following configurations
      client_url: 'http://192.168.1.2:50000/' # CosyVoice gradio demo webui url
      mode_checkbox_group: '预训练音色'
      sft_dropdown: '中文女'
      prompt_text: ''
      prompt_wav_upload_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav'
      prompt_wav_record_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav'
      instruct_text: ''
      seed: 0
      api_name: '/generate_audio'

    cosyvoice2_tts: # Cosy Voice TTS connects to the gradio webui
      # Check their documentation for deployment and the meaning of the following configurations
      client_url: 'http://192.168.1.2:50000/' # CosyVoice gradio demo webui url
      mode_checkbox_group: '3s极速复刻'
      sft_dropdown: ''
      prompt_text: ''
      prompt_wav_upload_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav'
      prompt_wav_record_url: 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav'
      instruct_text: ''
      stream: False
      seed: 0
      speed: 1.0
      api_name: '/generate_audio'

    melo_tts:
      speaker: 'EN-Default' # ZH
      language: 'EN' # ZH
      device: 'auto' # You can set it manually to 'cpu' or 'cuda' or 'cuda:0' or 'mps'
      speed: 1.0

    x_tts:
      api_url: 'http://192.168.1.2:8020/tts_to_audio'
      speaker_wav: 'female'
      language: 'en'

    gpt_sovits_tts:
      # put ref audio to root path of GPT-Sovits, or set the path here
      api_url: 'http://192.168.1.2:9880/tts'
      text_lang: 'zh'
      ref_audio_path: ''
      prompt_lang: 'zh'
      prompt_text: ''
      text_split_method: 'cut5'
      batch_size: '1'
      media_type: 'wav'
      streaming_mode: 'false'

    coqui_tts:
      # Name of the TTS model to use. If empty, will use default model
      # do 'tts --list_models' to list supported models for coqui-tts
      # Some examples:
      # - 'tts_models/en/ljspeech/tacotron2-DDC' (single speaker)
      # - 'tts_models/zh-CN/baker/tacotron2-DDC-GST' (single speaker for chinese)
      # - 'tts_models/multilingual/multi-dataset/your_tts' (multi-speaker)
      # - 'tts_models/multilingual/multi-dataset/xtts_v2' (multi-speaker)
      model_name: 'tts_models/en/ljspeech/tacotron2-DDC'
      speaker_wav: ''
      language: 'en'
      device: ''

    # pip install sherpa-onnx
    # documentation: https://k2-fsa.github.io/sherpa/onnx/index.html
    # TTS models download: https://github.com/k2-fsa/sherpa-onnx/releases/tag/tts-models
    # see config_alts for more examples
    sherpa_onnx_tts:
      vits_model: './models/vits-melo-tts-zh_en/model.onnx' # Path to VITS model file
      vits_lexicon: './models/vits-melo-tts-zh_en/lexicon.txt' # Path to lexicon file (optional)
      vits_tokens: './models/vits-melo-tts-zh_en/tokens.txt' # Path to tokens file
      vits_data_dir: '' # './models/vits-piper-en_GB-cori-high/espeak-ng-data'  # Path to espeak-ng data (optional)
      vits_dict_dir: './models/vits-melo-tts-zh_en/dict' # Path to Jieba dict (optional, for Chinese)
      tts_rule_fsts: './models/vits-melo-tts-zh_en/number.fst,models/vits-melo-tts-zh_en/phone.fst,models/vits-melo-tts-zh_en/date.fst,models/vits-melo-tts-zh_en/new_heteronym.fst' # Path to rule FSTs file (optional)
      max_num_sentences: 2 # Max sentences per batch (or -1 for all)
      sid: 1 # Speaker ID (for multi-speaker models)
      provider: 'cpu' # Use 'cpu', 'cuda' (GPU), or 'coreml' (Apple)
      num_threads: 1 # Number of computation threads
      speed: 1.0 # Speech speed (1.0 is normal)
      debug: false # Enable debug mode (True/False)

    spark_tts:
      api_url: 'http://192.168.1.2:6006/' # API URL. Uses Gradio's built-in front-end API. Repository: https://github.com/SparkAudio/Spark-TTS
      api_name:  "voice_clone" # Endpoint name. Options: voice_clone, voice_creation
      prompt_wav_upload: "https://uploadstatic.mihoyo.com/ys-obc/2022/11/02/16576950/4d9feb71760c5e8eb5f6c700df12fa0c_6824265537002152805.mp3" # Reference audio URL. Provide if api_name equals "voice_clone"
      gender:  "female" # Voice type (gender). Provide if api_name equals "voice_creation"
      pitch:  3 # Pitch shift (in semitones) default 3,range 1-5. Valid only if api_name equals "voice_creation"
      speed:  3 # Speed of the voice (in percent) default 3,range 1-5. Valid only if api_name equals "voice_creation"

    openai_tts: # Configuration for OpenAI-compatible TTS endpoints
      # These settings override the defaults in the openai_tts.py file if provided
      model: 'kokoro' # Model name expected by the server (e.g., 'tts-1', 'kokoro')
      voice: 'af_sky+af_bella' # Voice name(s) expected by the server (e.g., 'alloy', 'af_sky+af_bella')
      api_key: 'not-needed' # API key if required by the server
      base_url: 'http://192.168.1.2:8880/v1' # Base URL of the TTS server
      file_extension: 'mp3' # Audio file format ('mp3' or 'wav')

  # =================== Voice Activity Detection ===================
  vad_config:
    vad_model: null

    silero_vad:
      orig_sr: 16000 # Original Audio Sample Rate
      target_sr: 16000 # Target Audio Sample Rate
      prob_threshold: 0.4 # Probability Threshold for VAD
      db_threshold: 60 # Decibel Threshold for VAD
      required_hits: 3 # Number of consecutive hits required to consider speech
      required_misses: 24 # Number of consecutive misses required to consider silence
      smoothing_window: 5 # Smoothing window size for VAD

  tts_preprocessor_config:
    # settings regarding preprocessing for text that goes into TTS

    remove_special_char: True # remove special characters like emoji from audio generation
    ignore_brackets: True # ignore everything inside brackets
    ignore_parentheses: True # ignore everything inside parentheses
    ignore_asterisks: False # ignore everything wrapped inside asterisks
    ignore_angle_brackets: True # ignore everything wrapped inside <text>

